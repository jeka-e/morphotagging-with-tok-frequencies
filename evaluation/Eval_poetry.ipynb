{"cells":[{"cell_type":"markdown","metadata":{"id":"IUjZMZXtb89p"},"source":["# Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QmES4yFcEMAc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"67d85abe-cc93-489f-969f-f730358b9151","executionInfo":{"status":"ok","timestamp":1653442900053,"user_tz":-180,"elapsed":6446,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: deeppavlov in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.0.12)\n","Requirement already satisfied: rusenttokenize==0.0.5 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.0.5)\n","Requirement already satisfied: numpy==1.18.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.18.0)\n","Requirement already satisfied: overrides==2.7.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.7.0)\n","Requirement already satisfied: pyopenssl==22.0.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (22.0.0)\n","Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.21.2)\n","Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n","Requirement already satisfied: pytelegrambotapi==3.6.7 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.6.7)\n","Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.10.0)\n","Requirement already satisfied: tqdm==4.62.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (4.62.0)\n","Requirement already satisfied: pytz==2019.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2019.1)\n","Requirement already satisfied: fastapi==0.47.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.47.1)\n","Requirement already satisfied: ruamel.yaml==0.15.100 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.15.100)\n","Requirement already satisfied: pydantic==1.3 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.3)\n","Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.4.417127.4579844)\n","Requirement already satisfied: uvicorn==0.11.7 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.11.7)\n","Requirement already satisfied: uvloop==0.14.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.14.0)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n","Requirement already satisfied: sacremoses==0.0.35 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.0.35)\n","Requirement already satisfied: prometheus-client==0.7.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.7.1)\n","Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.22.0)\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.4.5)\n","Requirement already satisfied: Cython==0.29.14 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.29.14)\n","Requirement already satisfied: pymorphy2==0.8 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.8)\n","Requirement already satisfied: aio-pika==6.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (6.4.1)\n","Requirement already satisfied: pandas==0.25.3 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.25.3)\n","Requirement already satisfied: aiormq<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (3.3.1)\n","Requirement already satisfied: yarl in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (1.7.2)\n","Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /usr/local/lib/python3.7/dist-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deeppavlov) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n","Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n","Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n","Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n","Requirement already satisfied: cryptography>=35.0 in /usr/local/lib/python3.7/dist-packages (from pyopenssl==22.0.0->deeppavlov) (37.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2022.5.18.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->deeppavlov) (1.1.0)\n","Requirement already satisfied: httptools==0.1.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (0.1.2)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (0.9.0)\n","Requirement already satisfied: websockets==8.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (8.1)\n","Requirement already satisfied: pamqp==2.3.0 in /usr/local/lib/python3.7/dist-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (2.21)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (4.2.0)\n","Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (6.0.2)\n"]}],"source":["# %%capture\n","! pip install deeppavlov"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"034KTVnxF3XJ","executionInfo":{"status":"ok","timestamp":1653442951072,"user_tz":-180,"elapsed":51046,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["%%capture\n","!pip install tensorflow-gpu==1.15.2\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eU8RW0YGEVwp","executionInfo":{"status":"ok","timestamp":1653443005618,"user_tz":-180,"elapsed":54715,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["%%capture\n","! python -m deeppavlov install morpho_ru_syntagrus_bert\n","! pip install pyconll\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mcApbpo4chwC","executionInfo":{"status":"ok","timestamp":1653443005621,"user_tz":-180,"elapsed":53,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qooinGiRdBr6","outputId":"484db420-41bf-4e72-dd83-5bf025c02410","executionInfo":{"status":"ok","timestamp":1653443045360,"user_tz":-180,"elapsed":39778,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Data and model"],"metadata":{"id":"89m2jMJMcJky"}},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"EGRf5xGxcaHF"}},{"cell_type":"markdown","source":["Download data and save it to data folder"],"metadata":{"id":"SJCVFsWl2Gew"}},{"cell_type":"code","source":["%%capture\n","! wget https://raw.githubusercontent.com/dialogue-evaluation/GramEval2020/master/dataTrain/GramEval2020-Taiga-poetry-train.conllu\n","! wget https://raw.githubusercontent.com/dialogue-evaluation/GramEval2020/master/dataOpenTest/GramEval2020-Taiga-poetry-dev.conllu"],"metadata":{"id":"QSp0IJv8HKFD","executionInfo":{"status":"ok","timestamp":1653443045363,"user_tz":-180,"elapsed":37,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","os.makedirs('/content/data/GramEval/poetry')\n","os.rename(\"/content/GramEval2020-Taiga-poetry-train.conllu\", \"/content/data/GramEval/poetry/ru_poetry-ud-train.conllu\")\n","os.rename(\"/content/GramEval2020-Taiga-poetry-dev.conllu\", \"/content/data/GramEval/poetry/ru_poetry-ud-dev.conllu\")\n"],"metadata":{"id":"TP8FjD91HUK0","executionInfo":{"status":"ok","timestamp":1653443045368,"user_tz":-180,"elapsed":35,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["datafolder_path = '/content/data/GramEval/poetry' #path to datafolder\n","models_path = '/content/drive/MyDrive/models_diploma/GramEval_poetry'\n","language_conf = 'ru_poetry'"],"metadata":{"id":"oP6vNHYAzdn3","executionInfo":{"status":"ok","timestamp":1653443045370,"user_tz":-180,"elapsed":36,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## RuBERT"],"metadata":{"id":"YGxpVfS4ccbj"}},{"cell_type":"markdown","source":["download bert model, don't know why though"],"metadata":{"id":"4CvyP2TR2T4S"}},{"cell_type":"code","source":["%%capture\n","!wget http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz"],"metadata":{"id":"q9slJMLbv_nI","executionInfo":{"status":"ok","timestamp":1653443070380,"user_tz":-180,"elapsed":25044,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import tarfile\n","\n","file = tarfile.open('./rubert_cased_L-12_H-768_A-12_v1.tar.gz')\n","  \n","# extracting file\n","file.extractall('./downloads/bert_models')\n","  \n","file.close()"],"metadata":{"id":"A2mPT-j1weEV","executionInfo":{"status":"ok","timestamp":1653443073804,"user_tz":-180,"elapsed":3562,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsbMA3yqGtrX"},"source":["# Count and split tokens by frequency"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSIGI1kcGyGM"},"outputs":[],"source":["from utils import divide_by_freq\n","\n","top100, top1000, other = divide_by_freq(datafolder_path+'/ru_syntagrus-ud-train.conllu')\n","\n","os.makedirs(datafolder_path + '/freq_groups')\n","\n","for i, x in enumerate([top100, top1000, other]):\n","    file_path = datafolder_path + '/freq_groups' + '/{}.txt'.format(i)\n","    with open(file_path, 'w') as f:\n","        f.write(','.join(x))\n"]},{"cell_type":"markdown","metadata":{"id":"bb-UPIUPczCA"},"source":["# Components"]},{"cell_type":"markdown","metadata":{"id":"hcMJpEuZLzPh"},"source":["## Preprocessor"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYynb_4rKLUD","outputId":"1fce23ae-2302-450f-f6d9-acf900bae9ff","executionInfo":{"status":"ok","timestamp":1653443127876,"user_tz":-180,"elapsed":5686,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package perluniprops to /root/nltk_data...\n","[nltk_data]   Unzipping misc/perluniprops.zip.\n","[nltk_data] Downloading package nonbreaking_prefixes to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"]}],"source":["import re\n","import random\n","from logging import getLogger\n","from typing import Tuple, List, Optional, Union\n","\n","from deeppavlov.core.common.registry import register\n","from bert_dp.preprocessing import convert_examples_to_features, InputExample, InputFeatures\n","from bert_dp.tokenization import FullTokenizer\n","\n","from deeppavlov.core.commands.utils import expand_path\n","from deeppavlov.core.data.utils import zero_pad\n","from deeppavlov.core.models.component import Component\n","from deeppavlov.models.preprocessors.mask import Mask\n","\n","\n","@register('my_bert_ner_preprocessor')\n","class MyBertNerPreprocessor(Component):\n","    \"\"\"Takes tokens and splits them into bert subtokens, encodes subtokens with their indices.\n","    Creates a mask of subtokens (one for the first subtoken, zero for the others).\n","    If tags are provided, calculates tags for subtokens.\n","    Args:\n","        vocab_file: path to vocabulary\n","        do_lower_case: set True if lowercasing is needed\n","        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n","        max_subword_length: replace token to <unk> if it's length is larger than this\n","            (defaults to None, which is equal to +infinity)\n","        token_masking_prob: probability of masking token while training\n","        provide_subword_tags: output tags for subwords or for words\n","        subword_mask_mode: subword to select inside word tokens, can be \"first\" or \"last\"\n","            (default=\"first\")\n","    Attributes:\n","        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n","        max_subword_length: rmax lenght of a bert subtoken\n","        tokenizer: instance of Bert FullTokenizer\n","\n","    !!Added: \n","        topk_tokens: path to file with tokens to consider in topk_tokens_mask in train mode\n","        if list of files is passed,  condiser all the tokens apart from the ones included in the files\n","    \"\"\"\n","\n","    def __init__(self,\n","                 vocab_file: str,\n","                 topk_tokens_path: List[str] = None, # added\n","                 last: bool = False,\n","                 do_lower_case: bool = False,\n","                 max_seq_length: int = 512,\n","                 max_subword_length: int = None,\n","                 token_masking_prob: float = 0.0,\n","                 provide_subword_tags: bool = False,\n","                 subword_mask_mode: str = \"first\",\n","                 **kwargs):\n","        self._re_tokenizer = re.compile(r\"[\\w']+|[^\\w ]\")\n","        self.provide_subword_tags = provide_subword_tags\n","        self.mode = kwargs.get('mode')\n","        self.max_seq_length = max_seq_length\n","        self.max_subword_length = max_subword_length\n","        self.subword_mask_mode = subword_mask_mode\n","        vocab_file = str(expand_path(vocab_file))\n","        self.tokenizer = FullTokenizer(vocab_file=vocab_file,\n","                                       do_lower_case=do_lower_case)\n","        self.token_masking_prob = token_masking_prob\n","\n","        self.last = None # added\n","        self.topk_tokens = [] # added\n","        for filename in topk_tokens_path: # added\n","            with open(filename, 'r') as f: # added\n","                  tokens = f.read().split(',') # added\n","            self.topk_tokens.extend(tokens) # added\n","        \n","       \n","        self.last = last\n","        \n","\n","    def __call__(self,\n","                 tokens: Union[List[List[str]], List[str]],\n","                 tags: List[List[str]] = None,\n","                 **kwargs):\n","      \n","        if isinstance(tokens[0], str):\n","            tokens = [re.findall(self._re_tokenizer, s) for s in tokens]\n","        subword_tokens, subword_tok_ids, startofword_markers, subword_tags, topk_tok_mask = [], [], [], [], []\n","        for i in range(len(tokens)):\n","            toks = tokens[i]\n","            ys = ['O'] * len(toks) if tags is None else tags[i]\n","            assert len(toks) == len(ys), \\\n","                f\"toks({len(toks)}) should have the same length as ys({len(ys)})\"\n","            sw_toks, sw_marker, sw_ys, tk_masks = \\\n","                self._ner_bert_tokenize(toks,\n","                                        ys,\n","                                        self.topk_tokens, # added\n","                                        self.tokenizer,\n","                                        self.max_subword_length,\n","                                        mode=self.mode,\n","                                        last=self.last,\n","                                        subword_mask_mode=self.subword_mask_mode,\n","                                        token_masking_prob=self.token_masking_prob)\n","            if self.max_seq_length is not None:\n","                if len(sw_toks) > self.max_seq_length:\n","                    raise RuntimeError(f\"input sequence after bert tokenization\"\n","                                       f\" shouldn't exceed {self.max_seq_length} tokens.\")\n","            topk_tok_mask.append(tk_masks) # added\n","            subword_tokens.append(sw_toks)\n","            subword_tok_ids.append(self.tokenizer.convert_tokens_to_ids(sw_toks))\n","            startofword_markers.append(sw_marker)\n","            subword_tags.append(sw_ys)\n","            assert len(sw_marker) == len(sw_toks) == len(subword_tok_ids[-1]) == len(sw_ys), \\\n","                f\"length of sow_marker({len(sw_marker)}), tokens({len(sw_toks)}),\" \\\n","                f\" token ids({len(subword_tok_ids[-1])}) and ys({len(ys)})\" \\\n","                f\" for tokens = `{toks}` should match\"\n","\n","        subword_tok_ids = zero_pad(subword_tok_ids, dtype=int, padding=0)\n","        startofword_markers = zero_pad(startofword_markers, dtype=int, padding=0)\n","        attention_mask = Mask()(subword_tokens)\n","        topk_tok_mask = zero_pad(topk_tok_mask, dtype=int, padding=0) # added\n","\n","\n","        if tags is not None:\n","            if self.provide_subword_tags:\n","                return tokens, subword_tokens, subword_tok_ids, \\\n","                    attention_mask, startofword_markers, subword_tags\n","            else:\n","                nonmasked_tags = [[t for t in ts if t != 'X'] for ts in tags]\n","                for swts, swids, swms, ts in zip(subword_tokens,\n","                                                 subword_tok_ids,\n","                                                 startofword_markers,\n","                                                 nonmasked_tags):\n","                    if (len(swids) != len(swms)) or (len(ts) != sum(swms)):\n","                        log.warning('Not matching lengths of the tokenization!')\n","                        log.warning(f'Tokens len: {len(swts)}\\n Tokens: {swts}')\n","                        log.warning(f'Markers len: {len(swms)}, sum: {sum(swms)}')\n","                        log.warning(f'Masks: {swms}')\n","                        log.warning(f'Tags len: {len(ts)}\\n Tags: {ts}')\n","                return tokens, subword_tokens, subword_tok_ids, \\\n","                    attention_mask, startofword_markers, nonmasked_tags\n","\n","        return tokens, subword_tokens, subword_tok_ids, startofword_markers, attention_mask, topk_tok_mask\n","\n","    @staticmethod\n","    def _ner_bert_tokenize(tokens: List[str],\n","                           tags: List[str],\n","                           topk_tokens: List[str],\n","                           tokenizer: FullTokenizer,\n","                           max_subword_len: int = None,\n","                           mode: str = None,\n","                           last: str = None,\n","                           subword_mask_mode: str = \"first\",\n","                           token_masking_prob: float = None) -> Tuple[List[str], List[int], List[str]]:\n","        do_masking = (mode == 'train') and (token_masking_prob is not None)\n","        do_cutting = (max_subword_len is not None)\n","        tokens_subword = ['[CLS]']\n","        topk_tokens_mask = [] # added\n","        startofword_markers = [0]\n","        tags_subword = ['X']\n","        for token, tag in zip(tokens, tags):\n","            token_marker = int(tag != 'X')\n","            subwords = tokenizer.tokenize(token)\n","            if not subwords or (do_cutting and (len(subwords) > max_subword_len)):\n","                tokens_subword.append('[UNK]')\n","                startofword_markers.append(token_marker)\n","                tags_subword.append(tag)\n","            else:\n","                if do_masking and (random.random() < token_masking_prob):\n","                    tokens_subword.extend(['[MASK]'] * len(subwords))\n","                else:\n","                    tokens_subword.extend(subwords)\n","\n","                if subword_mask_mode == \"last\":\n","                    startofword_markers.extend([0] * (len(subwords) - 1) + [token_marker])\n","                else: #subword_mask_mode=first\n","                    startofword_markers.extend([token_marker] + [0] * (len(subwords) - 1))\n","\n","                if token.lower() in topk_tokens:  # added\n","                    if last == True:  # added\n","                        topk_tokens_mask.extend([0]) # added\n","                    else:\n","                        topk_tokens_mask.extend([1]) # added\n","                else:\n","                    if last == True:   # added\n","                        topk_tokens_mask.extend([1]) # added\n","                    else:\n","                        topk_tokens_mask.extend([0]) # added\n","\n","\n","                tags_subword.extend([tag] + ['X'] * (len(subwords) - 1))\n","                \n","        tokens_subword.append('[SEP]')\n","        # topk_tokens_mask.append(0)\n","        startofword_markers.append(0)\n","        tags_subword.append('X')\n","        return tokens_subword, startofword_markers, tags_subword, topk_tokens_mask\n"]},{"cell_type":"markdown","metadata":{"id":"Qa0czko8L2Iv"},"source":["## Tagger\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4uV-LZV1HTH_","outputId":"77656bb9-2857-4785-bd26-99d7278e1c7d","executionInfo":{"status":"ok","timestamp":1653443127879,"user_tz":-180,"elapsed":46,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"]}],"source":["from logging import getLogger\n","from typing import List, Union, Dict, Optional\n","\n","import numpy as np\n","import tensorflow as tf\n","from bert_dp.modeling import BertConfig, BertModel\n","from bert_dp.optimization import AdamWeightDecayOptimizer\n","\n","from deeppavlov.core.commands.utils import expand_path\n","from deeppavlov.core.common.registry import register\n","from deeppavlov.core.layers.tf_layers import bi_rnn\n","from deeppavlov.core.models.tf_model import LRScheduledTFModel\n","\n","log = getLogger(__name__)\n","\n","\n","def token_from_subtoken(units: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n","    \"\"\" Assemble token level units from subtoken level units\n","    Args:\n","        units: tf.Tensor of shape [batch_size, SUBTOKEN_seq_length, n_features]\n","        mask: mask of token beginnings. For example: for tokens\n","                [[``[CLS]`` ``My``, ``capybara``, ``[SEP]``],\n","                [``[CLS]`` ``Your``, ``aar``, ``##dvark``, ``is``, ``awesome``, ``[SEP]``]]\n","            the mask will be\n","                [[0, 1, 1, 0, 0, 0, 0],\n","                [0, 1, 1, 0, 1, 1, 0]]\n","    Returns:\n","        word_level_units: Units assembled from ones in the mask. For the\n","            example above this units will correspond to the following\n","                [[``My``, ``capybara``],\n","                [``Your`, ``aar``, ``is``, ``awesome``,]]\n","            the shape of this tensor will be [batch_size, TOKEN_seq_length, n_features]\n","    \"\"\"\n","    shape = tf.cast(tf.shape(units), tf.int64)\n","    batch_size = shape[0]\n","    nf = shape[2]\n","    nf_int = units.get_shape().as_list()[-1]\n","\n","    # number of TOKENS in each sentence\n","    token_seq_lengths = tf.cast(tf.reduce_sum(mask, 1), tf.int64)\n","    # for a matrix m =\n","    # [[1, 1, 1],\n","    #  [0, 1, 1],\n","    #  [1, 0, 0]]\n","    # it will be\n","    # [3, 2, 1]\n","\n","    n_words = tf.reduce_sum(token_seq_lengths)\n","    # n_words -> 6\n","\n","    max_token_seq_len = tf.cast(tf.reduce_max(token_seq_lengths), tf.int64)\n","    # max_token_seq_len -> 3\n","\n","    idxs = tf.where(mask)\n","    # for the matrix mentioned above\n","    # tf.where(mask) ->\n","    # [[0, 0],\n","    #  [0, 1]\n","    #  [0, 2],\n","    #  [1, 1],\n","    #  [1, 2]\n","    #  [2, 0]]\n","\n","    sample_ids_in_batch = tf.pad(idxs[:, 0], [[1, 0]])\n","    # for indices\n","    # [[0, 0],\n","    #  [0, 1]\n","    #  [0, 2],\n","    #  [1, 1],\n","    #  [1, 2],\n","    #  [2, 0]]\n","    # it is\n","    # [0, 0, 0, 0, 1, 1, 2]\n","    # padding is for computing change from one sample to another in the batch\n","\n","    a = tf.cast(tf.not_equal(sample_ids_in_batch[1:], sample_ids_in_batch[:-1]), tf.int64)\n","    # for the example above the result of this statement equals\n","    # [0, 0, 0, 1, 0, 1]\n","    # so data samples begin in 3rd and 5th positions (the indexes of ones)\n","\n","    # transforming sample start masks to the sample starts themselves\n","    q = a * tf.cast(tf.range(n_words), tf.int64)\n","    # [0, 0, 0, 3, 0, 5]\n","    count_to_substract = tf.pad(tf.boolean_mask(q, q), [(1, 0)])\n","    # [0, 3, 5]\n","\n","    new_word_indices = tf.cast(tf.range(n_words), tf.int64) - tf.gather(count_to_substract, tf.cumsum(a))\n","    # tf.range(n_words) -> [0, 1, 2, 3, 4, 5]\n","    # tf.cumsum(a) -> [0, 0, 0, 1, 1, 2]\n","    # tf.gather(count_to_substract, tf.cumsum(a)) -> [0, 0, 0, 3, 3, 5]\n","    # new_word_indices -> [0, 1, 2, 3, 4, 5] - [0, 0, 0, 3, 3, 5] = [0, 1, 2, 0, 1, 0]\n","    # new_word_indices is the concatenation of range(word_len(sentence))\n","    # for all sentences in units\n","\n","    n_total_word_elements = tf.cast(batch_size * max_token_seq_len, tf.int32)\n","    word_indices_flat = tf.cast(idxs[:, 0] * max_token_seq_len + new_word_indices, tf.int32)\n","    x_mask = tf.reduce_sum(tf.one_hot(word_indices_flat, n_total_word_elements), 0)\n","    x_mask = tf.cast(x_mask, tf.bool)\n","    # to get absolute indices we add max_token_seq_len:\n","    # idxs[:, 0] * max_token_seq_len -> [0, 0, 0, 1, 1, 2] * 2 = [0, 0, 0, 3, 3, 6]\n","    # word_indices_flat -> [0, 0, 0, 3, 3, 6] + [0, 1, 2, 0, 1, 0] = [0, 1, 2, 3, 4, 6]\n","    # total number of words in the batch (including paddings)\n","    # batch_size * max_token_seq_len -> 3 * 3 = 9\n","    # tf.one_hot(...) ->\n","    # [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","    #  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","    #  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","    #  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","    #  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","    #  [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n","    #  x_mask -> [1, 1, 1, 1, 1, 0, 1, 0, 0]\n","\n","    full_range = tf.cast(tf.range(batch_size * max_token_seq_len), tf.int32)\n","    # full_range -> [0, 1, 2, 3, 4, 5, 6, 7, 8]\n","    nonword_indices_flat = tf.boolean_mask(full_range, tf.math.logical_not(x_mask))\n","    # # y_idxs -> [5, 7, 8]\n","\n","    # get a sequence of units corresponding to the start subtokens of the words\n","    # size: [n_words, n_features]\n","    elements = tf.gather_nd(units, idxs)\n","\n","    # prepare zeros for paddings\n","    # size: [batch_size * TOKEN_seq_length - n_words, n_features]\n","    paddings = tf.zeros(tf.stack([tf.reduce_sum(max_token_seq_len - token_seq_lengths),\n","                                  nf], 0), tf.float32)\n","\n","    tensor_flat = tf.dynamic_stitch([word_indices_flat, nonword_indices_flat],\n","                                    [elements, paddings])\n","    # tensor_flat -> [x, x, x, x, x, 0, x, 0, 0]\n","\n","    tensor = tf.reshape(tensor_flat, tf.stack([batch_size, max_token_seq_len, nf_int], 0))\n","    # tensor -> [[x, x, x],\n","    #            [x, x, 0],\n","    #            [x, 0, 0]]\n","\n","    return tensor\n","\n","\n","@register('my_bert_sequence_network')\n","class MyBertSequenceNetwork(LRScheduledTFModel):\n","    \"\"\"\n","    Basic class for BERT-based sequential architectures.\n","    Args:\n","\n","        keep_prob: dropout keep_prob for non-Bert layers\n","        bert_config_file: path to Bert configuration file\n","        pretrained_bert: pretrained Bert checkpoint\n","        attention_probs_keep_prob: keep_prob for Bert self-attention layers\n","        hidden_keep_prob: keep_prob for Bert hidden layers\n","        encoder_layer_ids: list of averaged layers from Bert encoder (layer ids)\n","            optimizer: name of tf.train.* optimizer or None for `AdamWeightDecayOptimizer`\n","            weight_decay_rate: L2 weight decay for `AdamWeightDecayOptimizer`\n","        encoder_dropout: dropout probability of encoder output layer\n","        ema_decay: what exponential moving averaging to use for network parameters, value from 0.0 to 1.0.\n","            Values closer to 1.0 put weight on the parameters history and values closer to 0.0 corresponds put weight\n","            on the current parameters.\n","        ema_variables_on_cpu: whether to put EMA variables to CPU. It may save a lot of GPU memory\n","        freeze_embeddings: set True to not train input embeddings set True to\n","            not train input embeddings set True to not train input embeddings\n","        learning_rate: learning rate of BERT head\n","        bert_learning_rate: learning rate of BERT body\n","        min_learning_rate: min value of learning rate if learning rate decay is used\n","        learning_rate_drop_patience: how many validations with no improvements to wait\n","        learning_rate_drop_div: the divider of the learning rate after `learning_rate_drop_patience` unsuccessful\n","            validations\n","        load_before_drop: whether to load best model before dropping learning rate or not\n","        clip_norm: clip gradients by norm\n","    \"\"\"\n","\n","    def __init__(self,\n","                 keep_prob: float,\n","                 bert_config_file: str,\n","                 pretrained_bert: str = None,\n","                 attention_probs_keep_prob: float = None,\n","                 hidden_keep_prob: float = None,\n","                 encoder_layer_ids: List[int] = (-1,),\n","                 encoder_dropout: float = 0.0,\n","                 optimizer: str = None,\n","                 weight_decay_rate: float = 1e-6,\n","                 ema_decay: float = None,\n","                 ema_variables_on_cpu: bool = True,\n","                 freeze_embeddings: bool = False,\n","                 learning_rate: float = 1e-3,\n","                 bert_learning_rate: float = 2e-5,\n","                 min_learning_rate: float = 1e-07,\n","                 learning_rate_drop_patience: int = 20,\n","                 learning_rate_drop_div: float = 2.0,\n","                 load_before_drop: bool = True,\n","                 clip_norm: float = 1.0,\n","                 **kwargs) -> None:\n","        super().__init__(learning_rate=learning_rate,\n","                         learning_rate_drop_div=learning_rate_drop_div,\n","                         learning_rate_drop_patience=learning_rate_drop_patience,\n","                         load_before_drop=load_before_drop,\n","                         clip_norm=clip_norm,\n","                         **kwargs)\n","        self.keep_prob = keep_prob\n","        self.encoder_layer_ids = encoder_layer_ids\n","        self.encoder_dropout = encoder_dropout\n","        self.optimizer = optimizer\n","        self.weight_decay_rate = weight_decay_rate\n","        self.ema_decay = ema_decay\n","        self.ema_variables_on_cpu = ema_variables_on_cpu\n","        self.freeze_embeddings = freeze_embeddings\n","        self.bert_learning_rate_multiplier = bert_learning_rate / learning_rate\n","        self.min_learning_rate = min_learning_rate\n","\n","        self.bert_config = BertConfig.from_json_file(str(expand_path(bert_config_file)))\n","\n","        if attention_probs_keep_prob is not None:\n","            self.bert_config.attention_probs_dropout_prob = 1.0 - attention_probs_keep_prob\n","        if hidden_keep_prob is not None:\n","            self.bert_config.hidden_dropout_prob = 1.0 - hidden_keep_prob\n","\n","        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n","        self.sess_config.gpu_options.allow_growth = True\n","        self.sess = tf.Session(config=self.sess_config)\n","\n","        self._init_graph()\n","\n","        self._init_optimizer()\n","\n","        self.sess.run(tf.global_variables_initializer())\n","\n","        if pretrained_bert is not None:\n","            pretrained_bert = str(expand_path(pretrained_bert))\n","\n","            if tf.train.checkpoint_exists(pretrained_bert) \\\n","                    and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):\n","                log.info('[initializing model with Bert from {}]'.format(pretrained_bert))\n","                # Exclude optimizer and classification variables from saved variables\n","                var_list = self._get_saveable_variables(\n","                    exclude_scopes=('Optimizer', 'learning_rate', 'momentum', 'ner', 'EMA'))\n","                saver = tf.train.Saver(var_list)\n","                saver.restore(self.sess, pretrained_bert)\n","\n","        if self.load_path is not None:\n","            self.load()\n","\n","        if self.ema:\n","            self.sess.run(self.ema.init_op)\n","\n","    def _init_graph(self) -> None:\n","        self.seq_lengths = tf.reduce_sum(self.y_masks_ph, axis=1)\n","\n","        self.bert = BertModel(config=self.bert_config,\n","                              is_training=self.is_train_ph,\n","                              input_ids=self.input_ids_ph,\n","                              input_mask=self.input_masks_ph,\n","                              token_type_ids=self.token_types_ph,\n","                              use_one_hot_embeddings=False)\n","        with tf.variable_scope('ner'):\n","            layer_weights = tf.get_variable('layer_weights_',\n","                                            shape=len(self.encoder_layer_ids),\n","                                            initializer=tf.ones_initializer(),\n","                                            trainable=True)\n","            layer_mask = tf.ones_like(layer_weights)\n","            layer_mask = tf.nn.dropout(layer_mask, self.encoder_keep_prob_ph)\n","            layer_weights *= layer_mask\n","            # to prevent zero division\n","            mask_sum = tf.maximum(tf.reduce_sum(layer_mask), 1.0)\n","            layer_weights = tf.unstack(layer_weights / mask_sum)\n","            # TODO: may be stack and reduce_sum is faster\n","            units = sum(w * l for w, l in zip(layer_weights, self.encoder_layers()))\n","            units = tf.nn.dropout(units, keep_prob=self.keep_prob_ph)\n","            # print('init graph var scope ner')\n","        return units\n","\n","    def _get_tag_mask(self) -> tf.Tensor:\n","        \"\"\"\n","        Returns: tag_mask,\n","            a mask that selects positions corresponding to word tokens (not padding and `CLS`)\n","        \"\"\"\n","        max_length = tf.reduce_max(self.seq_lengths)\n","        one_hot_max_len = tf.one_hot(self.seq_lengths - 1, max_length)\n","        tag_mask = tf.cumsum(one_hot_max_len[:, ::-1], axis=1)[:, ::-1]\n","\n","        return tag_mask\n","\n","    def encoder_layers(self):\n","        \"\"\"\n","        Returns: the output of BERT layers specfied in ``self.encoder_layers_ids``\n","        \"\"\"\n","        return [self.bert.all_encoder_layers[i] for i in self.encoder_layer_ids]\n","\n","    def _init_placeholders(self) -> None:\n","        self.input_ids_ph = tf.placeholder(shape=(None, None),\n","                                           dtype=tf.int32,\n","                                           name='token_indices_ph')\n","        self.input_masks_ph = tf.placeholder(shape=(None, None),\n","                                             dtype=tf.int32,\n","                                             name='token_mask_ph')\n","        self.token_types_ph = \\\n","            tf.placeholder_with_default(tf.zeros_like(self.input_ids_ph, dtype=tf.int32),\n","                                        shape=self.input_ids_ph.shape,\n","                                        name='token_types_ph')\n","        self.learning_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='learning_rate_ph')\n","        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')\n","        self.encoder_keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='encoder_keep_prob_ph')\n","        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')\n","\n","    def _init_optimizer(self) -> None:\n","        with tf.variable_scope('Optimizer'):\n","            self.global_step = tf.get_variable('global_step',\n","                                               shape=[],\n","                                               dtype=tf.int32,\n","                                               initializer=tf.constant_initializer(0),\n","                                               trainable=False)\n","            # default optimizer for Bert is Adam with fixed L2 regularization\n","\n","        if self.optimizer is None:\n","            self.train_op = \\\n","                self.get_train_op(self.loss,\n","                                  learning_rate=self.learning_rate_ph,\n","                                  optimizer=AdamWeightDecayOptimizer,\n","                                  weight_decay_rate=self.weight_decay_rate,\n","                                  beta_1=0.9,\n","                                  beta_2=0.999,\n","                                  epsilon=1e-6,\n","                                  optimizer_scope_name='Optimizer',\n","                                  exclude_from_weight_decay=[\"LayerNorm\",\n","                                                             \"layer_norm\",\n","                                                             \"bias\",\n","                                                             \"EMA\"])\n","        else:\n","            self.train_op = self.get_train_op(self.loss,\n","                                              learning_rate=self.learning_rate_ph,\n","                                              optimizer_scope_name='Optimizer')\n","\n","        if self.optimizer is None:\n","            with tf.variable_scope('Optimizer'):\n","                new_global_step = self.global_step + 1\n","                self.train_op = tf.group(self.train_op, [self.global_step.assign(new_global_step)])\n","\n","        if self.ema_decay is not None:\n","            _vars = self._get_trainable_variables(exclude_scopes=[\"Optimizer\",\n","                                                                  \"LayerNorm\",\n","                                                                  \"layer_norm\",\n","                                                                  \"bias\",\n","                                                                  \"learning_rate\",\n","                                                                  \"momentum\"])\n","\n","            self.ema = ExponentialMovingAverage(self.ema_decay,\n","                                                variables_on_cpu=self.ema_variables_on_cpu)\n","            self.train_op = self.ema.build(self.train_op, _vars, name=\"EMA\")\n","        else:\n","            self.ema = None\n","\n","    def get_train_op(self, loss: tf.Tensor, learning_rate: Union[tf.Tensor, float], **kwargs) -> tf.Operation:\n","        assert \"learnable_scopes\" not in kwargs, \"learnable scopes unsupported\"\n","        # train_op for bert variables\n","        kwargs['learnable_scopes'] = ('bert/encoder', 'bert/embeddings')\n","        if self.freeze_embeddings:\n","            kwargs['learnable_scopes'] = ('bert/encoder',)\n","        bert_learning_rate = learning_rate * self.bert_learning_rate_multiplier\n","        bert_train_op = super().get_train_op(loss,\n","                                             bert_learning_rate,\n","                                             **kwargs)\n","        # train_op for ner head variables\n","        kwargs['learnable_scopes'] = ('ner',)\n","        # print('1 get train op')\n","        head_train_op = super().get_train_op(loss,\n","                                             learning_rate,\n","                                             **kwargs)\n","        return tf.group(bert_train_op, head_train_op)\n","\n","    def _build_basic_feed_dict(self, input_ids: tf.Tensor, input_masks: tf.Tensor,\n","                               token_types: Optional[tf.Tensor]=None, train: bool=False) -> dict:\n","        \"\"\"Fills the feed_dict with the tensors defined in the basic class.\n","        You need to update this dict by the values of output placeholders\n","        and class-specific network inputs in your derived class.\n","        \"\"\"\n","        # print('1 _build_basic_feed_dict')\n","        feed_dict = {\n","            self.input_ids_ph: input_ids,\n","            self.input_masks_ph: input_masks,\n","        }\n","        if token_types is not None:\n","            feed_dict[self.token_types_ph] = token_types\n","        if train:\n","            # print('1 _build_basic_feed_dict train')\n","            feed_dict.update({\n","                self.learning_rate_ph: max(self.get_learning_rate(), self.min_learning_rate),\n","                self.keep_prob_ph: self.keep_prob,\n","                self.encoder_keep_prob_ph: 1.0 - self.encoder_dropout,\n","                self.is_train_ph: True,\n","            })\n","\n","        return feed_dict\n","\n","    def _build_feed_dict(self, input_ids, input_masks, token_types=None, *args,  **kwargs):\n","        raise NotImplementedError(\"You must implement _build_feed_dict in your derived class.\")\n","\n","    def train_on_batch(self,\n","                       input_ids: Union[List[List[int]], np.ndarray],\n","                       input_masks: Union[List[List[int]], np.ndarray],\n","                      #  topk_tok_mask: Union[List[List[int]], np.ndarray]\n","                       *args, **kwargs) -> Dict[str, float]:\n","        \"\"\"\n","        Args:\n","            input_ids: batch of indices of subwords\n","            input_masks: batch of masks which determine what should be attended\n","            args: arguments passed  to _build_feed_dict\n","                and corresponding to additional input\n","                and output tensors of the derived class.\n","            kwargs: keyword arguments passed to _build_feed_dict\n","                and corresponding to additional input\n","                and output tensors of the derived class.\n","\n","        Returns:\n","            dict with fields 'loss', 'head_learning_rate', and 'bert_learning_rate'\n","        \"\"\"\n","        # print('1 tr_on_batch1')\n","        feed_dict = self._build_feed_dict(input_ids, input_masks, *args, **kwargs)\n","        # print('1 feed_dict_tr_batch')\n","        # print('1 tr_on_batch2')\n","        if self.ema:\n","            self.sess.run(self.ema.switch_to_train_op)\n","        # print('1 sess with loss')\n","        # print(self.topk_tok_mask)\n","        _, loss, lr = self.sess.run([self.train_op, self.loss, self.learning_rate_ph],\n","                                     feed_dict=feed_dict)\n","\n","        return {'loss': loss,\n","                'head_learning_rate': float(lr),\n","                'bert_learning_rate': float(lr) * self.bert_learning_rate_multiplier}\n","\n","    def __call__(self,\n","                 input_ids: Union[List[List[int]], np.ndarray],\n","                 input_masks: Union[List[List[int]], np.ndarray],\n","                 **kwargs) -> Union[List[List[int]], List[np.ndarray]]:\n","        raise NotImplementedError(\"You must implement method __call__ in your derived class.\")\n","\n","    def save(self, exclude_scopes=('Optimizer', 'EMA/BackupVariables')) -> None:\n","        if self.ema:\n","            self.sess.run(self.ema.switch_to_train_op)\n","        return super().save(exclude_scopes=exclude_scopes)\n","\n","    def load(self,\n","             exclude_scopes=('Optimizer',\n","                             'learning_rate',\n","                             'momentum',\n","                             'EMA/BackupVariables'),\n","             **kwargs) -> None:\n","        return super().load(exclude_scopes=exclude_scopes, **kwargs)\n","\n","\n","@register('my_bert_sequence_tagger')\n","class MyBertSequenceTagger(MyBertSequenceNetwork):\n","    \"\"\"BERT-based model for text tagging. It predicts a label for every token (not subtoken) in the text.\n","    You can use it for sequence labeling tasks, such as morphological tagging or named entity recognition.\n","    See :class:`deeppavlov.models.bert.bert_sequence_tagger.BertSequenceNetwork`\n","    for the description of inherited parameters.\n","    Args:\n","        n_tags: number of distinct tags\n","        use_crf: whether to use CRF on top or not\n","        use_birnn: whether to use bidirection rnn after BERT layers.\n","            For NER and morphological tagging we usually set it to `False` as otherwise the model overfits\n","        birnn_cell_type: the type of Bidirectional RNN. Either `lstm` or `gru`\n","        birnn_hidden_size: number of hidden units in the BiRNN layer in each direction\n","        return_probas: set this to `True` if you need the probabilities instead of raw answers\n","    \"\"\"\n","\n","    def __init__(self,\n","                 n_tags: List[str],\n","                 keep_prob: float,\n","                 bert_config_file: str,\n","                 pretrained_bert: str = None,\n","                 attention_probs_keep_prob: float = None,\n","                 hidden_keep_prob: float = None,\n","                 use_crf=False,\n","                 encoder_layer_ids: List[int] = (-1,),\n","                 encoder_dropout: float = 0.0,\n","                 optimizer: str = None,\n","                 weight_decay_rate: float = 1e-6,\n","                 use_birnn: bool = False,\n","                 birnn_cell_type: str = 'lstm',\n","                 birnn_hidden_size: int = 128,\n","                 ema_decay: float = None,\n","                 ema_variables_on_cpu: bool = True,\n","                 return_probas: bool = False,\n","                 freeze_embeddings: bool = False,\n","                 learning_rate: float = 1e-3,\n","                 bert_learning_rate: float = 2e-5,\n","                 min_learning_rate: float = 1e-07,\n","                 learning_rate_drop_patience: int = 20,\n","                 learning_rate_drop_div: float = 2.0,\n","                 load_before_drop: bool = True,\n","                 clip_norm: float = 1.0,\n","                 **kwargs) -> None:\n","        self.n_tags = n_tags\n","        self.use_crf = use_crf\n","        self.use_birnn = use_birnn\n","        self.birnn_cell_type = birnn_cell_type\n","        self.birnn_hidden_size = birnn_hidden_size\n","        self.return_probas = return_probas\n","        super().__init__(keep_prob=keep_prob,\n","                         bert_config_file=bert_config_file,\n","                         pretrained_bert=pretrained_bert,\n","                         attention_probs_keep_prob=attention_probs_keep_prob,\n","                         hidden_keep_prob=hidden_keep_prob,\n","                         encoder_layer_ids=encoder_layer_ids,\n","                         encoder_dropout=encoder_dropout,\n","                         optimizer=optimizer,\n","                         weight_decay_rate=weight_decay_rate,\n","                         ema_decay=ema_decay,\n","                         ema_variables_on_cpu=ema_variables_on_cpu,\n","                         freeze_embeddings=freeze_embeddings,\n","                         learning_rate=learning_rate,\n","                         bert_learning_rate=bert_learning_rate,\n","                         min_learning_rate=min_learning_rate,\n","                         learning_rate_drop_div=learning_rate_drop_div,\n","                         learning_rate_drop_patience=learning_rate_drop_patience,\n","                         load_before_drop=load_before_drop,\n","                         clip_norm=clip_norm,\n","                         **kwargs)\n","\n","    def _init_graph(self) -> None:\n","        self._init_placeholders()\n","\n","        units = super()._init_graph()\n","\n","        with tf.variable_scope('ner'):\n","            if self.use_birnn:\n","                units, _ = bi_rnn(units,\n","                                  self.birnn_hidden_size,\n","                                  cell_type=self.birnn_cell_type,\n","                                  seq_lengths=self.seq_lengths,\n","                                  name='birnn')\n","                units = tf.concat(units, -1)\n","            # TODO: maybe add one more layer?\n","            logits = tf.layers.dense(units, units=self.n_tags, name=\"output_dense\")\n","            self.logits = token_from_subtoken(logits, self.y_masks_ph) #token_ids of startwords?\n","\n","            # CRF\n","            if self.use_crf:\n","                transition_params = tf.get_variable('Transition_Params',\n","                                                    shape=[self.n_tags, self.n_tags],\n","                                                    initializer=tf.zeros_initializer())\n","                log_likelihood, transition_params = \\\n","                    tf.contrib.crf.crf_log_likelihood(self.logits,\n","                                                      self.y_ph,\n","                                                      self.seq_lengths,\n","                                                      transition_params)\n","                loss_tensor = -log_likelihood\n","                self._transition_params = transition_params\n","\n","            self.y_predictions = tf.argmax(self.logits, -1)\n","            self.y_probas = tf.nn.softmax(self.logits, axis=2)\n","\n","        with tf.variable_scope(\"loss\"):\n","            tag_mask = self._get_tag_mask()\n","            y_mask = tf.cast(tag_mask, tf.float32)\n","            if self.use_crf:\n","                self.loss = tf.reduce_mean(loss_tensor)\n","            else:                \n","                self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.y_ph,\n","                                                                   logits=self.logits,\n","                                                                   weights=self.topk_masks_ph)  #instead of y_mask\n","\n","\n","    def _init_placeholders(self) -> None:\n","        super()._init_placeholders()\n","        self.y_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='y_ph')\n","        self.y_masks_ph = tf.placeholder(shape=(None, None),\n","                                         dtype=tf.int32,\n","                                         name='y_mask_ph')\n","        self.topk_masks_ph = tf.placeholder(shape=(None, None),  # added\n","                                         dtype=tf.int32,\n","                                         name='topk_mask_ph')\n","\n","    def _decode_crf(self, feed_dict: Dict[tf.Tensor, np.ndarray]) -> List[np.ndarray]:\n","        logits, trans_params, mask, seq_lengths = self.sess.run([self.logits,\n","                                                                 self._transition_params,\n","                                                                 self.y_masks_ph,\n","                                                                 self.seq_lengths],\n","                                                                feed_dict=feed_dict)\n","        # iterate over the sentences because no batching in viterbi_decode\n","        y_pred = []\n","        for logit, sequence_length in zip(logits, seq_lengths):\n","            logit = logit[:int(sequence_length)]  # keep only the valid steps\n","            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n","            y_pred += [viterbi_seq]\n","        return y_pred\n","\n","    def _build_feed_dict(self, input_ids, input_masks, y_masks, topk_tok_mask, y=None):\n","        feed_dict = self._build_basic_feed_dict(input_ids, input_masks, train=(y is not None))\n","\n","        if y is not None:\n","            max_len = 0\n","            for seq in topk_tok_mask:\n","                if len(seq) > max_len:\n","                    max_len = len(seq)\n","            feed_dict[self.topk_masks_ph] = topk_tok_mask # added            \n","            feed_dict[self.y_ph] = y\n","        feed_dict[self.y_masks_ph] = y_masks\n","\n","        return feed_dict\n","\n","    def __call__(self,\n","                 input_ids: Union[List[List[int]], np.ndarray],\n","                 input_masks: Union[List[List[int]], np.ndarray],\n","                 y_masks: Union[List[List[int]], np.ndarray],\n","                 topk_tok_mask: Union[List[List[int]], np.ndarray]) -> Union[List[List[int]], List[np.ndarray]]:\n","        \"\"\" Predicts tag indices for a given subword tokens batch\n","        Args:\n","            input_ids: indices of the subwords\n","            input_masks: mask that determines where to attend and where not to\n","            y_masks: mask which determines the first subword units in the the word\n","\n","        Added: \n","            topk_tok_mask: mask that determines where to attend and where not to\n","        Returns:\n","            Label indices or class probabilities for each token (not subtoken)\n","        # \"\"\"\n","        \n","        feed_dict = self._build_feed_dict(input_ids, input_masks, y_masks, topk_tok_mask)\n","       \n","        if self.ema:\n","            self.sess.run(self.ema.switch_to_test_op)\n","        if not self.return_probas:\n","            if self.use_crf:\n","                pred = self._decode_crf(feed_dict)\n","            else:\n","                pred, seq_lengths = self.sess.run([self.y_predictions, self.seq_lengths], feed_dict=feed_dict)\n","                pred = [p[:l] for l, p in zip(seq_lengths, pred)]\n","        else:\n","            pred = self.sess.run(self.y_probas, feed_dict=feed_dict)\n","\n","        return pred\n","\n","\n","class ExponentialMovingAverage:\n","    def __init__(self,\n","                 decay: float = 0.999,\n","                 variables_on_cpu: bool = True) -> None:\n","        self.decay = decay\n","        self.ema = tf.train.ExponentialMovingAverage(decay=decay)\n","        self.var_device_name = '/cpu:0' if variables_on_cpu else None\n","        self.train_mode = None\n","\n","    def build(self,\n","              minimize_op: tf.Tensor,\n","              update_vars: List[tf.Variable] = None,\n","              name: str = \"EMA\") -> tf.Tensor:\n","        with tf.variable_scope(name):\n","            if update_vars is None:\n","                update_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n","\n","            with tf.control_dependencies([minimize_op]):\n","                minimize_op = self.ema.apply(update_vars)\n","\n","            with tf.device(self.var_device_name):\n","                # Make backup variables\n","                with tf.variable_scope('BackupVariables'):\n","                    backup_vars = [tf.get_variable(var.op.name,\n","                                                   dtype=var.value().dtype,\n","                                                   trainable=False,\n","                                                   initializer=var.initialized_value())\n","                                   for var in update_vars]\n","\n","                def ema_to_weights():\n","                    return tf.group(*(tf.assign(var, self.ema.average(var).read_value())\n","                                      for var in update_vars))\n","\n","                def save_weight_backups():\n","                    return tf.group(*(tf.assign(bck, var.read_value())\n","                                      for var, bck in zip(update_vars, backup_vars)))\n","\n","                def restore_weight_backups():\n","                    return tf.group(*(tf.assign(var, bck.read_value())\n","                                      for var, bck in zip(update_vars, backup_vars)))\n","\n","                train_switch_op = restore_weight_backups()\n","                with tf.control_dependencies([save_weight_backups()]):\n","                    test_switch_op = ema_to_weights()\n","\n","            self.train_switch_op = train_switch_op\n","            self.test_switch_op = test_switch_op\n","            self.do_nothing_op = tf.no_op()\n","\n","        return minimize_op\n","\n","    @property\n","    def init_op(self) -> tf.Operation:\n","        self.train_mode = False\n","        return self.test_switch_op\n","\n","    @property\n","    def switch_to_train_op(self) -> tf.Operation:\n","        assert self.train_mode is not None, \"ema variables aren't initialized\"\n","        if not self.train_mode:\n","            # log.info(\"switching to train mode\")\n","            self.train_mode = True\n","            return self.train_switch_op\n","        return self.do_nothing_op\n","\n","    @property\n","    def switch_to_test_op(self) -> tf.Operation:\n","        assert self.train_mode is not None, \"ema variables aren't initialized\"\n","        if self.train_mode:\n","            # log.info(\"switching to test mode\")\n","            self.train_mode = False\n","            return self.test_switch_op\n","        return self.do_nothing_op"]},{"cell_type":"markdown","metadata":{"id":"ylWGF63EOycm"},"source":["## Return Error"]},{"cell_type":"markdown","metadata":{"id":"kckPvFmJHITU"},"source":["Component that masks tokens for the evaluation, so the metrics are counted considering certain group of tokens. By masking here I mean just forming new lists of tokens without the irrelevant ones.\n","\n","Also it forms 3 files in the models folder:\n","- correct.txt - all the correctly tagged tokens, token/predicted/sentence\n","- error.txt - all the wrongly tagged tokens, token/predicted/true/sentence\n","- count.txt - number of tokens, to compute accuracy later, overall_tokens/correct_tokens\n","- preds.txt - predictions of the classifier\n","\n","Not elegant at all, but was a little limited by deeppavlov's functions arrangement"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"cq15SHY4Oyj3","executionInfo":{"status":"ok","timestamp":1653443564886,"user_tz":-180,"elapsed":47,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["@register('Info_logger')\n","class Info_logger(Component):\n","    def __init__(self, info_path, **kwargs):\n","        os.makedirs(info_path, exist_ok=True)\n","\n","        self.error_path = info_path + '/errors.txt'\n","        self.correct_path = info_path + '/correct.txt'\n","        self.count_path = info_path + '/count.txt'\n","        self.preds_path = info_path + '/preds.txt'\n","\n","\n","    def __call__(self, y, topk_tok_mask, y_predicted, x_words, *args, **kwargs):\n","\n","        error_file = open(self.error_path, 'a')  # token/predicted/true/sentence\n","        correct_file = open(self.correct_path, 'a') # token/predicted/sentence\n","        count_file = open(self.count_path, 'a') # overall_tokens/correct_tokens\n","        preds_file = open(self.preds_path, 'a') #\n","\n","        y_predicted_masked = []\n","        y_masked = []\n","        overall_tokens = 0\n","        correct_tokens = 0\n","\n","        for i in range(len(y_predicted)):\n","            line_preds = []\n","            line_true = []\n","            if x_words[i] != []:\n","                sentence = ' '.join(x_words[i])\n","            for j in range(len(y_predicted[i])):\n","                if topk_tok_mask[i][j] == 1:\n","                    overall_tokens += 1\n","                    line_preds.append(y_predicted[i][j])\n","                    line_true.append(y[i][j])\n","                    preds_file.write(x_words[i][j] + '\\t' + y_predicted[i][j] + '\\t' + sentence + '\\n')\n","                    if y_predicted[i][j] == y[i][j]:\n","                        correct_tokens += 1\n","                        correct_file.write(x_words[i][j] + '\\t' + y[i][j] + '\\t' + sentence + '\\n')\n","                    else:\n","                        error_file.write(x_words[i][j] + '\\t' + y_predicted[i][j] + '\\t' + y[i][j] + '\\t' + sentence + '\\n')\n","\n","            y_predicted_masked.append(np.array(line_preds))\n","            y_masked.append(np.array(line_true))\n","\n","        \n","        count_file.write(str(overall_tokens)+'\\t'+str(correct_tokens)+'\\n')\n","\n","        error_file.close()\n","        correct_file.close()\n","        count_file.close()\n","        # ic (y_predicted_masked)\n","        # ic (y_masked)\n","        return y_masked, y_predicted_masked\n","        "]},{"cell_type":"markdown","metadata":{"id":"4XSodQDbIMDo"},"source":["# Evaluation"]},{"cell_type":"markdown","metadata":{"id":"KU4ZAVpFI2rY"},"source":["need to specify in config:\n","- ['dataset_reader']['data_path'] - path to datafolder\n","- ['metadata']['variables']['MODELS_PATH'] - path to general folder concerning to certain dataset\n","- ['train']['evaluation_targets'] = valid, ['dataset_reader']['data_types'] = ['train', 'dev'], if there is no test file\n","- ['metadata']['variables']['DATA_PATH'] - path to datafolder\n","\n","will be specified below:\n","- ['metadata']['variables']['WORK_PATH'] - path to certain model folder\n","- ['chainer']['pipe'][1]['topk_tokens_path'] - path to the group of tokens to consider"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"9PDJDAESgeFo","executionInfo":{"status":"ok","timestamp":1653443445830,"user_tz":-180,"elapsed":414,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["#START \n","# alter config4eval from the newest base_config "]},{"cell_type":"code","execution_count":22,"metadata":{"id":"d-rIu6PaIb6J","executionInfo":{"status":"ok","timestamp":1653443446393,"user_tz":-180,"elapsed":3,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["import pandas as pd\n","\n","import json\n","from deeppavlov import build_model, configs, evaluate_model"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Pl_MCZVLJDHH","executionInfo":{"status":"ok","timestamp":1653443447041,"user_tz":-180,"elapsed":15,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval = json.load(open('./config_eval.json'))\n","\n","config_eval['metadata']['variables']['DATA_PATH'] = datafolder_path\n","config_eval['metadata']['variables']['MODELS_PATH'] = '{ROOT_PATH}/drive/MyDrive/models_diploma/GramEval_poetry'\n","# config_eval['train']['evaluation_targets'] = ['valid']\n","# config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","config_eval['dataset_reader']['language'] = language_conf\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"brn2DqhROlZV","executionInfo":{"status":"ok","timestamp":1653443451208,"user_tz":-180,"elapsed":358,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["def count_tokens(f):\n","\n","    overall_tokens, correct_tokens = 0, 0\n","\n","    for line in f.readlines():\n","        overall_tokens += int(line.split('\\t')[0])\n","        correct_tokens += int(line.split('\\t')[1])\n","\n","    return overall_tokens, correct_tokens\n","\n","def full_report(path, data_type='valid'): #path to models folder\n","\n","    top100_f = open(path + '/model_top100/info/' + data_type + '/count.txt', 'r')\n","    top1000_f = open(path + '/model_top1000/info/' + data_type + '/count.txt', 'r')\n","    other_f = open(path + '/model_other/info/' + data_type + '/count.txt', 'r')\n","\n","    rez = {'model': ['top100', 'top1000', 'other', 'general'], \n","           'accuracy': [],\n","            }\n","\n","    total, correct = 0, 0\n","    for f in [top100_f, top1000_f, other_f]:\n","         overall_tokens, correct_tokens = count_tokens(f)\n","         rez['accuracy'].append(round(correct_tokens/overall_tokens, 4))\n","         total += overall_tokens\n","         correct += correct_tokens\n","    \n","    rez['accuracy'].append(round(correct/total, 4))\n","    \n","    rez_df = pd.DataFrame(rez)\n","\n","    top100_f.close()\n","    top1000_f.close()\n","    other_f.close()\n","\n","    return rez_df\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QWEQJYVINGRw"},"source":["## From Scratch Evaluation"]},{"cell_type":"markdown","metadata":{"id":"WGw5clwYNOh-"},"source":["###top100"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"TAUrzLFtNOiG","executionInfo":{"status":"ok","timestamp":1653443568967,"user_tz":-180,"elapsed":336,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/fs_models/model_top100'\n","config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt']\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","\n","config_eval['chainer']['pipe'][1]['last'] = False\n","config_eval['chainer']['pipe'][7]['info_path'] = '{WORK_PATH}/info/valid'"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLR9riGqNZZ3","outputId":"8bd6f17c-c5c7-45f2-8e48-39f6b5bd0520","executionInfo":{"status":"ok","timestamp":1653443591319,"user_tz":-180,"elapsed":20858,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:52:50.230 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:52:50.610 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_top100/tag.dict]\n","2022-05-25 01:53:05.41 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_top100/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:53:06.739 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"bKzJhMZsO_K7"},"source":["###top1000"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"bNPCXIc7O_K8","executionInfo":{"status":"ok","timestamp":1653443641648,"user_tz":-180,"elapsed":457,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/fs_models/model_top1000'\n","config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/1.txt']\n","\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","config_eval['chainer']['pipe'][1]['last'] = False\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{WORK_PATH}/info/valid'"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0Qov_ZVPGpc","outputId":"99bf1de1-d0d2-4e49-9106-60938377ee0b","executionInfo":{"status":"ok","timestamp":1653443674543,"user_tz":-180,"elapsed":32463,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:54:01.496 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:54:02.265 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_top1000/tag.dict]\n","2022-05-25 01:54:18.464 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_top1000/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:54:24.666 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"NPknOeRDPqIn"},"source":["###other"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"EGXs9NE6PqIt","executionInfo":{"status":"ok","timestamp":1653443674562,"user_tz":-180,"elapsed":238,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/fs_models/model_other'\n","config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt', '{MODELS_PATH}/freq_groups/1.txt']\n","config_eval['chainer']['pipe'][1]['last'] = True\n","\n","\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{WORK_PATH}/info/valid'"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObNi8lc0PsjU","outputId":"1ab4777e-2dc9-4ab0-899d-6a0153cebb5b","executionInfo":{"status":"ok","timestamp":1653443696754,"user_tz":-180,"elapsed":22425,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:54:25.963 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:54:26.532 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n","2022-05-25 01:54:42.726 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models/model_other/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:54:51.32 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"zAufr1NtQ_rp"},"source":["### Report"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"QptwhK3CRDsy","executionInfo":{"status":"ok","timestamp":1653443696756,"user_tz":-180,"elapsed":154,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["path = '/content/drive/MyDrive/models_diploma/GramEval_poetry/fs_models' #maybe wrong\n","rez_val = full_report(path, data_type='valid')"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"wA6mIRqURDsy","outputId":"9c842d4b-44dc-4ed7-f947-115a3a5df577","executionInfo":{"status":"ok","timestamp":1653443696757,"user_tz":-180,"elapsed":132,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     model  accuracy\n","0   top100    0.9933\n","1  top1000    0.9860\n","2    other    0.9895\n","3  general    0.9898"],"text/html":["\n","  <div id=\"df-bd47733d-1a56-46d0-a2b9-afecbc3b2cff\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>top100</td>\n","      <td>0.9933</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>top1000</td>\n","      <td>0.9860</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>other</td>\n","      <td>0.9895</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>general</td>\n","      <td>0.9898</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd47733d-1a56-46d0-a2b9-afecbc3b2cff')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bd47733d-1a56-46d0-a2b9-afecbc3b2cff button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bd47733d-1a56-46d0-a2b9-afecbc3b2cff');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":35}],"source":["rez_val"]},{"cell_type":"markdown","metadata":{"id":"VWXLLHp_Iqg0"},"source":["## Baseline Evaluation "]},{"cell_type":"markdown","metadata":{"id":"ZcZXHVy2m6Fn"},"source":["### Baseline on 100"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"2wx5KCFKpLgo","executionInfo":{"status":"ok","timestamp":1653443696759,"user_tz":-180,"elapsed":55,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/baseline'\n","config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt']\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","config_eval['chainer']['pipe'][1]['last'] = False\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{WORK_PATH}/model_top100/info/valid'"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZemM5Y70s_N","outputId":"7dca9b4f-28bc-4015-fabe-47fe056517ec","executionInfo":{"status":"ok","timestamp":1653443723412,"user_tz":-180,"elapsed":26707,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:54:52.527 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:54:53.67 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/tag.dict]\n","2022-05-25 01:55:08.566 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:55:14.731 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)\n"]},{"cell_type":"markdown","metadata":{"id":"NslNUycZ9Qf_"},"source":["### Baseline 1000"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"urFgjRPu9Si_","executionInfo":{"status":"ok","timestamp":1653443723419,"user_tz":-180,"elapsed":77,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/1.txt']\n","config_eval['chainer']['pipe'][7]['info_path'] = '{WORK_PATH}/model_top1000'\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","config_eval['chainer']['pipe'][1]['last'] = False\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{WORK_PATH}/model_top1000/info/valid'"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xf0CsX59SlU","outputId":"a60e4777-a83f-4358-9806-b1e54b5e7838","executionInfo":{"status":"ok","timestamp":1653443740498,"user_tz":-180,"elapsed":17152,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:55:16.92 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:55:16.463 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/tag.dict]\n","2022-05-25 01:55:33.473 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:55:35.306 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"BcHMu7qL9Y3i"},"source":["### Baseline 1000+"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"Ms2DmmVX9ZA6","executionInfo":{"status":"ok","timestamp":1653443740500,"user_tz":-180,"elapsed":23,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt', '{MODELS_PATH}/freq_groups/1.txt']\n","config_eval['chainer']['pipe'][1]['last'] = True\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{WORK_PATH}/model_other/info/valid'"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hXCQj-H9ZD_","outputId":"6829e561-eb91-4a5f-c066-08b9cb34055a","executionInfo":{"status":"ok","timestamp":1653443758631,"user_tz":-180,"elapsed":18145,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:55:36.661 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:55:37.16 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/tag.dict]\n","2022-05-25 01:55:53.542 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:55:55.288 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"EAYO_5OIepQf"},"source":["### Report"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"5OlOANGFUBwT","executionInfo":{"status":"ok","timestamp":1653443758636,"user_tz":-180,"elapsed":56,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["path = '/content/drive/MyDrive/models_diploma/GramEval_poetry/baseline' #maybe wrong\n","rez = full_report(path, data_type='valid')"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"6PpM43lRl_B3","outputId":"d18c6411-1019-415a-cdb0-be1e79ba126d","executionInfo":{"status":"ok","timestamp":1653443758639,"user_tz":-180,"elapsed":44,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     model  accuracy\n","0   top100    0.9933\n","1  top1000    0.9860\n","2    other    0.9895\n","3  general    0.9898"],"text/html":["\n","  <div id=\"df-214ba045-af1b-40d5-ab77-2145a8433211\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>top100</td>\n","      <td>0.9933</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>top1000</td>\n","      <td>0.9860</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>other</td>\n","      <td>0.9895</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>general</td>\n","      <td>0.9898</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-214ba045-af1b-40d5-ab77-2145a8433211')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-214ba045-af1b-40d5-ab77-2145a8433211 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-214ba045-af1b-40d5-ab77-2145a8433211');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":43}],"source":["rez"]},{"cell_type":"markdown","metadata":{"id":"gwq4a3i7KJD7"},"source":["## FineTuned Model Evaluation =  baseline as it did not improve"]},{"cell_type":"markdown","metadata":{"id":"v5-w4K-BZAiz"},"source":["###top100"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"S6DAFgWYZAiz","executionInfo":{"status":"ok","timestamp":1653443758641,"user_tz":-180,"elapsed":41,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/baseline'\n","config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt']\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","config_eval['chainer']['pipe'][1]['last'] = False\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{MODELS_PATH}/finetuned_models/model_top100/info/valid'"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9dBZp_kZAiz","outputId":"7d55d6ff-66ba-4555-faa6-b167f63e3ab9","executionInfo":{"status":"ok","timestamp":1653443778564,"user_tz":-180,"elapsed":19961,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:55:56.678 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:55:57.39 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/tag.dict]\n","2022-05-25 01:56:13.758 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:56:15.547 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"D_oo55qRZAi0"},"source":["###top1000"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"gNxh9q5RZAi0","executionInfo":{"status":"ok","timestamp":1653443778566,"user_tz":-180,"elapsed":107,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/baseline'\n","config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/1.txt']\n","\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","config_eval['chainer']['pipe'][1]['last'] = False\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{MODELS_PATH}/finetuned_models/model_top1000/info/valid'"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63n4RqxCZAi0","outputId":"e1475564-6ca2-4b6e-fab5-c2cec50e6e05","executionInfo":{"status":"ok","timestamp":1653443799883,"user_tz":-180,"elapsed":21417,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:56:17.931 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:56:18.282 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/tag.dict]\n","2022-05-25 01:56:34.617 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:56:36.396 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"a4beD7twZAi0"},"source":["###other"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"w9Kvh4NkZAi0","executionInfo":{"status":"ok","timestamp":1653443799890,"user_tz":-180,"elapsed":77,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["config_eval['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/baseline'\n","config_eval['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt', '{MODELS_PATH}/freq_groups/1.txt']\n","config_eval['chainer']['pipe'][1]['last'] = True\n","\n","\n","config_eval['train']['evaluation_targets'] = ['valid']\n","config_eval['dataset_reader']['data_types'] = ['train', 'dev']\n","\n","config_eval['chainer']['pipe'][7]['info_path'] = '{MODELS_PATH}/finetuned_models/model_other/info/valid'"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q887CvOeZAi1","outputId":"8e4112f7-5b40-44d1-82b5-9d73e2f6c923","executionInfo":{"status":"ok","timestamp":1653443818640,"user_tz":-180,"elapsed":18821,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 01:56:37.952 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n","2022-05-25 01:56:38.303 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/tag.dict]\n","2022-05-25 01:56:54.692 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model]\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/models_diploma/GramEval_poetry/baseline/model\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-25 01:56:56.467 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"]},{"output_type":"stream","name":"stdout","text":["{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\"}}\n"]}],"source":["model = evaluate_model(config_eval, download=False)"]},{"cell_type":"markdown","metadata":{"id":"OEFXlrJkZAi1"},"source":["### Report"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"0mBNMXCEZAi1","executionInfo":{"status":"ok","timestamp":1653443818655,"user_tz":-180,"elapsed":135,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[],"source":["path = '/content/drive/MyDrive/models_diploma/GramEval_poetry/finetuned_models' #maybe wrong\n","rez_val = full_report(path, data_type='valid')"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"1VRf7vQLZAi1","outputId":"76042537-0764-4164-de45-da9cc6563534","executionInfo":{"status":"ok","timestamp":1653443818662,"user_tz":-180,"elapsed":135,"user":{"displayName":"Zhenya Egorova","userId":"03866717224318332006"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     model  accuracy\n","0   top100    0.9933\n","1  top1000    0.9860\n","2    other    0.9895\n","3  general    0.9906"],"text/html":["\n","  <div id=\"df-d57d2fbe-59f5-4c42-80cd-767a1f3c80d6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>top100</td>\n","      <td>0.9933</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>top1000</td>\n","      <td>0.9860</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>other</td>\n","      <td>0.9895</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>general</td>\n","      <td>0.9906</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d57d2fbe-59f5-4c42-80cd-767a1f3c80d6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d57d2fbe-59f5-4c42-80cd-767a1f3c80d6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d57d2fbe-59f5-4c42-80cd-767a1f3c80d6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":51}],"source":["rez_val"]}],"metadata":{"colab":{"collapsed_sections":["IUjZMZXtb89p","89m2jMJMcJky","EGRf5xGxcaHF","xsbMA3yqGtrX","hcMJpEuZLzPh","Qa0czko8L2Iv","QWEQJYVINGRw","WGw5clwYNOh-","bKzJhMZsO_K7","NPknOeRDPqIn","zAufr1NtQ_rp","VWXLLHp_Iqg0","ZcZXHVy2m6Fn","NslNUycZ9Qf_","BcHMu7qL9Y3i","D_oo55qRZAi0"],"name":"Eval_poetry.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}