{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "IUjZMZXtb89p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmES4yFcEMAc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install deeppavlov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "034KTVnxF3XJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow-gpu==1.15.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eU8RW0YGEVwp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! python -m deeppavlov install morpho_ru_syntagrus_bert\n",
        "! pip install pyconll\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install icecream"
      ],
      "metadata": {
        "id": "Gx8hhIJUcTZ1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from icecream import ic\n",
        "import os"
      ],
      "metadata": {
        "id": "mcApbpo4chwC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qooinGiRdBr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bfa306-2437-4dc7-d2ce-c65a0744ec8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and model"
      ],
      "metadata": {
        "id": "89m2jMJMcJky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "EGRf5xGxcaHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download data and save it to data folder"
      ],
      "metadata": {
        "id": "SJCVFsWl2Gew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! wget https://raw.githubusercontent.com/dialogue-evaluation/GramEval2020/master/dataTrain/GramEval2020-Taiga-poetry-train.conllu\n",
        "! wget https://raw.githubusercontent.com/dialogue-evaluation/GramEval2020/master/dataOpenTest/GramEval2020-Taiga-poetry-dev.conllu"
      ],
      "metadata": {
        "id": "QSp0IJv8HKFD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/data/GramEval/poetry')\n",
        "os.rename(\"/content/GramEval2020-Taiga-poetry-train.conllu\", \"/content/data/GramEval/poetry/ru_poetry-ud-train.conllu\")\n",
        "os.rename(\"/content/GramEval2020-Taiga-poetry-dev.conllu\", \"/content/data/GramEval/poetry/ru_poetry-ud-dev.conllu\")\n"
      ],
      "metadata": {
        "id": "TP8FjD91HUK0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafolder_path = '/content/data/GramEval/poetry' #path to datafolder\n",
        "models_path = '/content/drive/MyDrive/models_diploma/GramEval_poetry'\n",
        "language_conf = 'ru_poetry'"
      ],
      "metadata": {
        "id": "oP6vNHYAzdn3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RuBERT"
      ],
      "metadata": {
        "id": "YGxpVfS4ccbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download bert model, don't know why though"
      ],
      "metadata": {
        "id": "4CvyP2TR2T4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz"
      ],
      "metadata": {
        "id": "q9slJMLbv_nI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "file = tarfile.open('./rubert_cased_L-12_H-768_A-12_v1.tar.gz')\n",
        "  \n",
        "# extracting file\n",
        "file.extractall('./downloads/bert_models')\n",
        "  \n",
        "file.close()"
      ],
      "metadata": {
        "id": "A2mPT-j1weEV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count and split tokens by frequency"
      ],
      "metadata": {
        "id": "Br3NWTQEcnXL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RY5Ou00JEc3t"
      },
      "outputs": [],
      "source": [
        "from utils import divide_by_freq\n",
        "\n",
        "top100, top1000, other = divide_by_freq(datafolder_path+'/ru_poetry-ud-train.conllu')\n",
        "\n",
        "os.makedirs(models_path + '/freq_groups')\n",
        "\n",
        "for i, x in enumerate([top100, top1000, other]):\n",
        "    file_path = models_path + '/freq_groups' + '/{}.txt'.format(i)\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(','.join(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Components"
      ],
      "metadata": {
        "id": "bb-UPIUPczCA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcMJpEuZLzPh"
      },
      "source": [
        "## Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bYynb_4rKLUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5422e9f4-309b-40c4-b34c-fb440fc31e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Package perluniprops is already up-to-date!\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from logging import getLogger\n",
        "from typing import Tuple, List, Optional, Union\n",
        "\n",
        "from deeppavlov.core.common.registry import register\n",
        "from bert_dp.preprocessing import convert_examples_to_features, InputExample, InputFeatures\n",
        "from bert_dp.tokenization import FullTokenizer\n",
        "\n",
        "from deeppavlov.core.commands.utils import expand_path\n",
        "from deeppavlov.core.data.utils import zero_pad\n",
        "from deeppavlov.core.models.component import Component\n",
        "from deeppavlov.models.preprocessors.mask import Mask\n",
        "\n",
        "\n",
        "@register('my_bert_ner_preprocessor')\n",
        "class MyBertNerPreprocessor(Component):\n",
        "    \"\"\"Takes tokens and splits them into bert subtokens, encodes subtokens with their indices.\n",
        "    Creates a mask of subtokens (one for the first subtoken, zero for the others).\n",
        "    If tags are provided, calculates tags for subtokens.\n",
        "    Args:\n",
        "        vocab_file: path to vocabulary\n",
        "        do_lower_case: set True if lowercasing is needed\n",
        "        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n",
        "        max_subword_length: replace token to <unk> if it's length is larger than this\n",
        "            (defaults to None, which is equal to +infinity)\n",
        "        token_masking_prob: probability of masking token while training\n",
        "        provide_subword_tags: output tags for subwords or for words\n",
        "        subword_mask_mode: subword to select inside word tokens, can be \"first\" or \"last\"\n",
        "            (default=\"first\")\n",
        "    Attributes:\n",
        "        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n",
        "        max_subword_length: rmax lenght of a bert subtoken\n",
        "        tokenizer: instance of Bert FullTokenizer\n",
        "\n",
        "    !!Added: \n",
        "        topk_tokens: path to file with tokens to consider in topk_tokens_mask in train mode\n",
        "        if list of files is passed,  condiser all the tokens apart from the ones included in the files\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_file: str,\n",
        "                 topk_tokens_path: List[str] = None, # added\n",
        "                 last: bool = False,\n",
        "                 do_lower_case: bool = False,\n",
        "                 max_seq_length: int = 512,\n",
        "                 max_subword_length: int = None,\n",
        "                 token_masking_prob: float = 0.0,\n",
        "                 provide_subword_tags: bool = False,\n",
        "                 subword_mask_mode: str = \"first\",\n",
        "                 **kwargs):\n",
        "        self._re_tokenizer = re.compile(r\"[\\w']+|[^\\w ]\")\n",
        "        self.provide_subword_tags = provide_subword_tags\n",
        "        self.mode = kwargs.get('mode')\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.max_subword_length = max_subword_length\n",
        "        self.subword_mask_mode = subword_mask_mode\n",
        "        vocab_file = str(expand_path(vocab_file))\n",
        "        self.tokenizer = FullTokenizer(vocab_file=vocab_file,\n",
        "                                       do_lower_case=do_lower_case)\n",
        "        self.token_masking_prob = token_masking_prob\n",
        "\n",
        "        self.last = None # added\n",
        "        self.topk_tokens = [] # added\n",
        "        for filename in topk_tokens_path: # added\n",
        "            with open(filename, 'r') as f: # added\n",
        "                  tokens = f.read().split(',') # added\n",
        "            self.topk_tokens.extend(tokens) # added\n",
        "        \n",
        "        \n",
        "        self.last = last\n",
        "        \n",
        "\n",
        "    def __call__(self,\n",
        "                 tokens: Union[List[List[str]], List[str]],\n",
        "                 tags: List[List[str]] = None,\n",
        "                 **kwargs):\n",
        "      \n",
        "        if isinstance(tokens[0], str):\n",
        "            tokens = [re.findall(self._re_tokenizer, s) for s in tokens]\n",
        "        subword_tokens, subword_tok_ids, startofword_markers, subword_tags, topk_tok_mask = [], [], [], [], []\n",
        "        for i in range(len(tokens)):\n",
        "            toks = tokens[i]\n",
        "            ys = ['O'] * len(toks) if tags is None else tags[i]\n",
        "            assert len(toks) == len(ys), \\\n",
        "                f\"toks({len(toks)}) should have the same length as ys({len(ys)})\"\n",
        "            sw_toks, sw_marker, sw_ys, tk_masks = \\\n",
        "                self._ner_bert_tokenize(toks,\n",
        "                                        ys,\n",
        "                                        self.topk_tokens, # added\n",
        "                                        self.tokenizer,\n",
        "                                        self.max_subword_length,\n",
        "                                        mode=self.mode,\n",
        "                                        last=self.last,\n",
        "                                        subword_mask_mode=self.subword_mask_mode,\n",
        "                                        token_masking_prob=self.token_masking_prob)\n",
        "            if self.max_seq_length is not None:\n",
        "                if len(sw_toks) > self.max_seq_length:\n",
        "                    raise RuntimeError(f\"input sequence after bert tokenization\"\n",
        "                                       f\" shouldn't exceed {self.max_seq_length} tokens.\")\n",
        "            topk_tok_mask.append(tk_masks) # added\n",
        "            subword_tokens.append(sw_toks)\n",
        "            subword_tok_ids.append(self.tokenizer.convert_tokens_to_ids(sw_toks))\n",
        "            startofword_markers.append(sw_marker)\n",
        "            subword_tags.append(sw_ys)\n",
        "            assert len(sw_marker) == len(sw_toks) == len(subword_tok_ids[-1]) == len(sw_ys), \\\n",
        "                f\"length of sow_marker({len(sw_marker)}), tokens({len(sw_toks)}),\" \\\n",
        "                f\" token ids({len(subword_tok_ids[-1])}) and ys({len(ys)})\" \\\n",
        "                f\" for tokens = `{toks}` should match\"\n",
        "\n",
        "        subword_tok_ids = zero_pad(subword_tok_ids, dtype=int, padding=0)\n",
        "        startofword_markers = zero_pad(startofword_markers, dtype=int, padding=0)\n",
        "        attention_mask = Mask()(subword_tokens)\n",
        "        topk_tok_mask = zero_pad(topk_tok_mask, dtype=int, padding=0) # added\n",
        "\n",
        "\n",
        "        if tags is not None:\n",
        "            if self.provide_subword_tags:\n",
        "                return tokens, subword_tokens, subword_tok_ids, \\\n",
        "                    attention_mask, startofword_markers, subword_tags\n",
        "            else:\n",
        "                nonmasked_tags = [[t for t in ts if t != 'X'] for ts in tags]\n",
        "                for swts, swids, swms, ts in zip(subword_tokens,\n",
        "                                                 subword_tok_ids,\n",
        "                                                 startofword_markers,\n",
        "                                                 nonmasked_tags):\n",
        "                    if (len(swids) != len(swms)) or (len(ts) != sum(swms)):\n",
        "                        log.warning('Not matching lengths of the tokenization!')\n",
        "                        log.warning(f'Tokens len: {len(swts)}\\n Tokens: {swts}')\n",
        "                        log.warning(f'Markers len: {len(swms)}, sum: {sum(swms)}')\n",
        "                        log.warning(f'Masks: {swms}')\n",
        "                        log.warning(f'Tags len: {len(ts)}\\n Tags: {ts}')\n",
        "                return tokens, subword_tokens, subword_tok_ids, \\\n",
        "                    attention_mask, startofword_markers, nonmasked_tags\n",
        "\n",
        "        return tokens, subword_tokens, subword_tok_ids, startofword_markers, attention_mask, topk_tok_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _ner_bert_tokenize(tokens: List[str],\n",
        "                           tags: List[str],\n",
        "                           topk_tokens: List[str],\n",
        "                           tokenizer: FullTokenizer,\n",
        "                           max_subword_len: int = None,\n",
        "                           mode: str = None,\n",
        "                           last: str = None,\n",
        "                           subword_mask_mode: str = \"first\",\n",
        "                           token_masking_prob: float = None) -> Tuple[List[str], List[int], List[str]]:\n",
        "        do_masking = (mode == 'train') and (token_masking_prob is not None)\n",
        "        do_cutting = (max_subword_len is not None)\n",
        "        tokens_subword = ['[CLS]']\n",
        "        topk_tokens_mask = [] # added\n",
        "        startofword_markers = [0]\n",
        "        tags_subword = ['X']\n",
        "        for token, tag in zip(tokens, tags):\n",
        "            token_marker = int(tag != 'X')\n",
        "            subwords = tokenizer.tokenize(token)\n",
        "            if not subwords or (do_cutting and (len(subwords) > max_subword_len)):\n",
        "                tokens_subword.append('[UNK]')\n",
        "                startofword_markers.append(token_marker)\n",
        "                tags_subword.append(tag)\n",
        "            else:\n",
        "                if do_masking and (random.random() < token_masking_prob):\n",
        "                    tokens_subword.extend(['[MASK]'] * len(subwords))\n",
        "                else:\n",
        "                    tokens_subword.extend(subwords)\n",
        "\n",
        "                if subword_mask_mode == \"last\":\n",
        "                    startofword_markers.extend([0] * (len(subwords) - 1) + [token_marker])\n",
        "                else: #subword_mask_mode=first\n",
        "                    startofword_markers.extend([token_marker] + [0] * (len(subwords) - 1))\n",
        "\n",
        "                if token.lower() in topk_tokens:  # added\n",
        "                    if last == True:  # added\n",
        "                        topk_tokens_mask.extend([0]) # added\n",
        "                    else:\n",
        "                        topk_tokens_mask.extend([1]) # added\n",
        "                else:\n",
        "                    if last == True:   # added\n",
        "                        topk_tokens_mask.extend([1]) # added\n",
        "                    else:\n",
        "                        topk_tokens_mask.extend([0]) # added\n",
        "\n",
        "\n",
        "                tags_subword.extend([tag] + ['X'] * (len(subwords) - 1))\n",
        "                \n",
        "        tokens_subword.append('[SEP]')\n",
        "        # topk_tokens_mask.append(0)\n",
        "        startofword_markers.append(0)\n",
        "        tags_subword.append('X')\n",
        "        return tokens_subword, startofword_markers, tags_subword, topk_tokens_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa0czko8L2Iv"
      },
      "source": [
        "## Tagger\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uV-LZV1HTH_",
        "outputId": "5a80fc7a-4f67-45e0-c3fc-c8b81b8a25c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from logging import getLogger\n",
        "from typing import List, Union, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from bert_dp.modeling import BertConfig, BertModel\n",
        "from bert_dp.optimization import AdamWeightDecayOptimizer\n",
        "\n",
        "from deeppavlov.core.commands.utils import expand_path\n",
        "from deeppavlov.core.common.registry import register\n",
        "from deeppavlov.core.layers.tf_layers import bi_rnn\n",
        "from deeppavlov.core.models.tf_model import LRScheduledTFModel\n",
        "\n",
        "log = getLogger(__name__)\n",
        "\n",
        "\n",
        "def token_from_subtoken(units: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\" Assemble token level units from subtoken level units\n",
        "    Args:\n",
        "        units: tf.Tensor of shape [batch_size, SUBTOKEN_seq_length, n_features]\n",
        "        mask: mask of token beginnings. For example: for tokens\n",
        "                [[``[CLS]`` ``My``, ``capybara``, ``[SEP]``],\n",
        "                [``[CLS]`` ``Your``, ``aar``, ``##dvark``, ``is``, ``awesome``, ``[SEP]``]]\n",
        "            the mask will be\n",
        "                [[0, 1, 1, 0, 0, 0, 0],\n",
        "                [0, 1, 1, 0, 1, 1, 0]]\n",
        "    Returns:\n",
        "        word_level_units: Units assembled from ones in the mask. For the\n",
        "            example above this units will correspond to the following\n",
        "                [[``My``, ``capybara``],\n",
        "                [``Your`, ``aar``, ``is``, ``awesome``,]]\n",
        "            the shape of this tensor will be [batch_size, TOKEN_seq_length, n_features]\n",
        "    \"\"\"\n",
        "    shape = tf.cast(tf.shape(units), tf.int64)\n",
        "    batch_size = shape[0]\n",
        "    nf = shape[2]\n",
        "    nf_int = units.get_shape().as_list()[-1]\n",
        "\n",
        "    # number of TOKENS in each sentence\n",
        "    token_seq_lengths = tf.cast(tf.reduce_sum(mask, 1), tf.int64)\n",
        "    # for a matrix m =\n",
        "    # [[1, 1, 1],\n",
        "    #  [0, 1, 1],\n",
        "    #  [1, 0, 0]]\n",
        "    # it will be\n",
        "    # [3, 2, 1]\n",
        "\n",
        "    n_words = tf.reduce_sum(token_seq_lengths)\n",
        "    # n_words -> 6\n",
        "\n",
        "    max_token_seq_len = tf.cast(tf.reduce_max(token_seq_lengths), tf.int64)\n",
        "    # max_token_seq_len -> 3\n",
        "\n",
        "    idxs = tf.where(mask)\n",
        "    # for the matrix mentioned above\n",
        "    # tf.where(mask) ->\n",
        "    # [[0, 0],\n",
        "    #  [0, 1]\n",
        "    #  [0, 2],\n",
        "    #  [1, 1],\n",
        "    #  [1, 2]\n",
        "    #  [2, 0]]\n",
        "\n",
        "    sample_ids_in_batch = tf.pad(idxs[:, 0], [[1, 0]])\n",
        "    # for indices\n",
        "    # [[0, 0],\n",
        "    #  [0, 1]\n",
        "    #  [0, 2],\n",
        "    #  [1, 1],\n",
        "    #  [1, 2],\n",
        "    #  [2, 0]]\n",
        "    # it is\n",
        "    # [0, 0, 0, 0, 1, 1, 2]\n",
        "    # padding is for computing change from one sample to another in the batch\n",
        "\n",
        "    a = tf.cast(tf.not_equal(sample_ids_in_batch[1:], sample_ids_in_batch[:-1]), tf.int64)\n",
        "    # for the example above the result of this statement equals\n",
        "    # [0, 0, 0, 1, 0, 1]\n",
        "    # so data samples begin in 3rd and 5th positions (the indexes of ones)\n",
        "\n",
        "    # transforming sample start masks to the sample starts themselves\n",
        "    q = a * tf.cast(tf.range(n_words), tf.int64)\n",
        "    # [0, 0, 0, 3, 0, 5]\n",
        "    count_to_substract = tf.pad(tf.boolean_mask(q, q), [(1, 0)])\n",
        "    # [0, 3, 5]\n",
        "\n",
        "    new_word_indices = tf.cast(tf.range(n_words), tf.int64) - tf.gather(count_to_substract, tf.cumsum(a))\n",
        "    # tf.range(n_words) -> [0, 1, 2, 3, 4, 5]\n",
        "    # tf.cumsum(a) -> [0, 0, 0, 1, 1, 2]\n",
        "    # tf.gather(count_to_substract, tf.cumsum(a)) -> [0, 0, 0, 3, 3, 5]\n",
        "    # new_word_indices -> [0, 1, 2, 3, 4, 5] - [0, 0, 0, 3, 3, 5] = [0, 1, 2, 0, 1, 0]\n",
        "    # new_word_indices is the concatenation of range(word_len(sentence))\n",
        "    # for all sentences in units\n",
        "\n",
        "    n_total_word_elements = tf.cast(batch_size * max_token_seq_len, tf.int32)\n",
        "    word_indices_flat = tf.cast(idxs[:, 0] * max_token_seq_len + new_word_indices, tf.int32)\n",
        "    x_mask = tf.reduce_sum(tf.one_hot(word_indices_flat, n_total_word_elements), 0)\n",
        "    x_mask = tf.cast(x_mask, tf.bool)\n",
        "    # to get absolute indices we add max_token_seq_len:\n",
        "    # idxs[:, 0] * max_token_seq_len -> [0, 0, 0, 1, 1, 2] * 2 = [0, 0, 0, 3, 3, 6]\n",
        "    # word_indices_flat -> [0, 0, 0, 3, 3, 6] + [0, 1, 2, 0, 1, 0] = [0, 1, 2, 3, 4, 6]\n",
        "    # total number of words in the batch (including paddings)\n",
        "    # batch_size * max_token_seq_len -> 3 * 3 = 9\n",
        "    # tf.one_hot(...) ->\n",
        "    # [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "    #  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
        "    #  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
        "    #  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
        "    #  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
        "    #  [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
        "    #  x_mask -> [1, 1, 1, 1, 1, 0, 1, 0, 0]\n",
        "\n",
        "    full_range = tf.cast(tf.range(batch_size * max_token_seq_len), tf.int32)\n",
        "    # full_range -> [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "    nonword_indices_flat = tf.boolean_mask(full_range, tf.math.logical_not(x_mask))\n",
        "    # # y_idxs -> [5, 7, 8]\n",
        "\n",
        "    # get a sequence of units corresponding to the start subtokens of the words\n",
        "    # size: [n_words, n_features]\n",
        "    elements = tf.gather_nd(units, idxs)\n",
        "\n",
        "    # prepare zeros for paddings\n",
        "    # size: [batch_size * TOKEN_seq_length - n_words, n_features]\n",
        "    paddings = tf.zeros(tf.stack([tf.reduce_sum(max_token_seq_len - token_seq_lengths),\n",
        "                                  nf], 0), tf.float32)\n",
        "\n",
        "    tensor_flat = tf.dynamic_stitch([word_indices_flat, nonword_indices_flat],\n",
        "                                    [elements, paddings])\n",
        "    # tensor_flat -> [x, x, x, x, x, 0, x, 0, 0]\n",
        "\n",
        "    tensor = tf.reshape(tensor_flat, tf.stack([batch_size, max_token_seq_len, nf_int], 0))\n",
        "    # tensor -> [[x, x, x],\n",
        "    #            [x, x, 0],\n",
        "    #            [x, 0, 0]]\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "@register('my_bert_sequence_network')\n",
        "class MyBertSequenceNetwork(LRScheduledTFModel):\n",
        "    \"\"\"\n",
        "    Basic class for BERT-based sequential architectures.\n",
        "    Args:\n",
        "\n",
        "        keep_prob: dropout keep_prob for non-Bert layers\n",
        "        bert_config_file: path to Bert configuration file\n",
        "        pretrained_bert: pretrained Bert checkpoint\n",
        "        attention_probs_keep_prob: keep_prob for Bert self-attention layers\n",
        "        hidden_keep_prob: keep_prob for Bert hidden layers\n",
        "        encoder_layer_ids: list of averaged layers from Bert encoder (layer ids)\n",
        "            optimizer: name of tf.train.* optimizer or None for `AdamWeightDecayOptimizer`\n",
        "            weight_decay_rate: L2 weight decay for `AdamWeightDecayOptimizer`\n",
        "        encoder_dropout: dropout probability of encoder output layer\n",
        "        ema_decay: what exponential moving averaging to use for network parameters, value from 0.0 to 1.0.\n",
        "            Values closer to 1.0 put weight on the parameters history and values closer to 0.0 corresponds put weight\n",
        "            on the current parameters.\n",
        "        ema_variables_on_cpu: whether to put EMA variables to CPU. It may save a lot of GPU memory\n",
        "        freeze_embeddings: set True to not train input embeddings set True to\n",
        "            not train input embeddings set True to not train input embeddings\n",
        "        learning_rate: learning rate of BERT head\n",
        "        bert_learning_rate: learning rate of BERT body\n",
        "        min_learning_rate: min value of learning rate if learning rate decay is used\n",
        "        learning_rate_drop_patience: how many validations with no improvements to wait\n",
        "        learning_rate_drop_div: the divider of the learning rate after `learning_rate_drop_patience` unsuccessful\n",
        "            validations\n",
        "        load_before_drop: whether to load best model before dropping learning rate or not\n",
        "        clip_norm: clip gradients by norm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 keep_prob: float,\n",
        "                 bert_config_file: str,\n",
        "                 pretrained_bert: str = None,\n",
        "                 attention_probs_keep_prob: float = None,\n",
        "                 hidden_keep_prob: float = None,\n",
        "                 encoder_layer_ids: List[int] = (-1,),\n",
        "                 encoder_dropout: float = 0.0,\n",
        "                 optimizer: str = None,\n",
        "                 weight_decay_rate: float = 1e-6,\n",
        "                 ema_decay: float = None,\n",
        "                 ema_variables_on_cpu: bool = True,\n",
        "                 freeze_embeddings: bool = False,\n",
        "                 learning_rate: float = 1e-3,\n",
        "                 bert_learning_rate: float = 2e-5,\n",
        "                 min_learning_rate: float = 1e-07,\n",
        "                 learning_rate_drop_patience: int = 20,\n",
        "                 learning_rate_drop_div: float = 2.0,\n",
        "                 load_before_drop: bool = True,\n",
        "                 clip_norm: float = 1.0,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(learning_rate=learning_rate,\n",
        "                         learning_rate_drop_div=learning_rate_drop_div,\n",
        "                         learning_rate_drop_patience=learning_rate_drop_patience,\n",
        "                         load_before_drop=load_before_drop,\n",
        "                         clip_norm=clip_norm,\n",
        "                         **kwargs)\n",
        "        self.keep_prob = keep_prob\n",
        "        self.encoder_layer_ids = encoder_layer_ids\n",
        "        self.encoder_dropout = encoder_dropout\n",
        "        self.optimizer = optimizer\n",
        "        self.weight_decay_rate = weight_decay_rate\n",
        "        self.ema_decay = ema_decay\n",
        "        self.ema_variables_on_cpu = ema_variables_on_cpu\n",
        "        self.freeze_embeddings = freeze_embeddings\n",
        "        self.bert_learning_rate_multiplier = bert_learning_rate / learning_rate\n",
        "        self.min_learning_rate = min_learning_rate\n",
        "\n",
        "        self.bert_config = BertConfig.from_json_file(str(expand_path(bert_config_file)))\n",
        "\n",
        "        if attention_probs_keep_prob is not None:\n",
        "            self.bert_config.attention_probs_dropout_prob = 1.0 - attention_probs_keep_prob\n",
        "        if hidden_keep_prob is not None:\n",
        "            self.bert_config.hidden_dropout_prob = 1.0 - hidden_keep_prob\n",
        "\n",
        "        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
        "        self.sess_config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.Session(config=self.sess_config)\n",
        "\n",
        "        self._init_graph()\n",
        "\n",
        "        self._init_optimizer()\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        if pretrained_bert is not None:\n",
        "            pretrained_bert = str(expand_path(pretrained_bert))\n",
        "\n",
        "            if tf.train.checkpoint_exists(pretrained_bert) \\\n",
        "                    and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):\n",
        "                log.info('[initializing model with Bert from {}]'.format(pretrained_bert))\n",
        "                # Exclude optimizer and classification variables from saved variables\n",
        "                var_list = self._get_saveable_variables(\n",
        "                    exclude_scopes=('Optimizer', 'learning_rate', 'momentum', 'ner', 'EMA'))\n",
        "                saver = tf.train.Saver(var_list)\n",
        "                saver.restore(self.sess, pretrained_bert)\n",
        "\n",
        "        if self.load_path is not None:\n",
        "            self.load()\n",
        "\n",
        "        if self.ema:\n",
        "            self.sess.run(self.ema.init_op)\n",
        "\n",
        "    def _init_graph(self) -> None:\n",
        "        self.seq_lengths = tf.reduce_sum(self.y_masks_ph, axis=1)\n",
        "\n",
        "        self.bert = BertModel(config=self.bert_config,\n",
        "                              is_training=self.is_train_ph,\n",
        "                              input_ids=self.input_ids_ph,\n",
        "                              input_mask=self.input_masks_ph,\n",
        "                              token_type_ids=self.token_types_ph,\n",
        "                              use_one_hot_embeddings=False)\n",
        "        with tf.variable_scope('ner'):\n",
        "            layer_weights = tf.get_variable('layer_weights_',\n",
        "                                            shape=len(self.encoder_layer_ids),\n",
        "                                            initializer=tf.ones_initializer(),\n",
        "                                            trainable=True)\n",
        "            layer_mask = tf.ones_like(layer_weights)\n",
        "            layer_mask = tf.nn.dropout(layer_mask, self.encoder_keep_prob_ph)\n",
        "            layer_weights *= layer_mask\n",
        "            # to prevent zero division\n",
        "            mask_sum = tf.maximum(tf.reduce_sum(layer_mask), 1.0)\n",
        "            layer_weights = tf.unstack(layer_weights / mask_sum)\n",
        "            # TODO: may be stack and reduce_sum is faster\n",
        "            units = sum(w * l for w, l in zip(layer_weights, self.encoder_layers()))\n",
        "            units = tf.nn.dropout(units, keep_prob=self.keep_prob_ph)\n",
        "            # print('init graph var scope ner')\n",
        "        return units\n",
        "\n",
        "    def _get_tag_mask(self) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Returns: tag_mask,\n",
        "            a mask that selects positions corresponding to word tokens (not padding and `CLS`)\n",
        "        \"\"\"\n",
        "        max_length = tf.reduce_max(self.seq_lengths)\n",
        "        one_hot_max_len = tf.one_hot(self.seq_lengths - 1, max_length)\n",
        "        tag_mask = tf.cumsum(one_hot_max_len[:, ::-1], axis=1)[:, ::-1]\n",
        "\n",
        "        return tag_mask\n",
        "\n",
        "    def encoder_layers(self):\n",
        "        \"\"\"\n",
        "        Returns: the output of BERT layers specfied in ``self.encoder_layers_ids``\n",
        "        \"\"\"\n",
        "        return [self.bert.all_encoder_layers[i] for i in self.encoder_layer_ids]\n",
        "\n",
        "    def _init_placeholders(self) -> None:\n",
        "        self.input_ids_ph = tf.placeholder(shape=(None, None),\n",
        "                                           dtype=tf.int32,\n",
        "                                           name='token_indices_ph')\n",
        "        self.input_masks_ph = tf.placeholder(shape=(None, None),\n",
        "                                             dtype=tf.int32,\n",
        "                                             name='token_mask_ph')\n",
        "        self.token_types_ph = \\\n",
        "            tf.placeholder_with_default(tf.zeros_like(self.input_ids_ph, dtype=tf.int32),\n",
        "                                        shape=self.input_ids_ph.shape,\n",
        "                                        name='token_types_ph')\n",
        "        self.learning_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='learning_rate_ph')\n",
        "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')\n",
        "        self.encoder_keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='encoder_keep_prob_ph')\n",
        "        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')\n",
        "\n",
        "    def _init_optimizer(self) -> None:\n",
        "        with tf.variable_scope('Optimizer'):\n",
        "            self.global_step = tf.get_variable('global_step',\n",
        "                                               shape=[],\n",
        "                                               dtype=tf.int32,\n",
        "                                               initializer=tf.constant_initializer(0),\n",
        "                                               trainable=False)\n",
        "            # default optimizer for Bert is Adam with fixed L2 regularization\n",
        "\n",
        "        if self.optimizer is None:\n",
        "            self.train_op = \\\n",
        "                self.get_train_op(self.loss,\n",
        "                                  learning_rate=self.learning_rate_ph,\n",
        "                                  optimizer=AdamWeightDecayOptimizer,\n",
        "                                  weight_decay_rate=self.weight_decay_rate,\n",
        "                                  beta_1=0.9,\n",
        "                                  beta_2=0.999,\n",
        "                                  epsilon=1e-6,\n",
        "                                  optimizer_scope_name='Optimizer',\n",
        "                                  exclude_from_weight_decay=[\"LayerNorm\",\n",
        "                                                             \"layer_norm\",\n",
        "                                                             \"bias\",\n",
        "                                                             \"EMA\"])\n",
        "        else:\n",
        "            self.train_op = self.get_train_op(self.loss,\n",
        "                                              learning_rate=self.learning_rate_ph,\n",
        "                                              optimizer_scope_name='Optimizer')\n",
        "\n",
        "        if self.optimizer is None:\n",
        "            with tf.variable_scope('Optimizer'):\n",
        "                new_global_step = self.global_step + 1\n",
        "                self.train_op = tf.group(self.train_op, [self.global_step.assign(new_global_step)])\n",
        "\n",
        "        if self.ema_decay is not None:\n",
        "            _vars = self._get_trainable_variables(exclude_scopes=[\"Optimizer\",\n",
        "                                                                  \"LayerNorm\",\n",
        "                                                                  \"layer_norm\",\n",
        "                                                                  \"bias\",\n",
        "                                                                  \"learning_rate\",\n",
        "                                                                  \"momentum\"])\n",
        "\n",
        "            self.ema = ExponentialMovingAverage(self.ema_decay,\n",
        "                                                variables_on_cpu=self.ema_variables_on_cpu)\n",
        "            self.train_op = self.ema.build(self.train_op, _vars, name=\"EMA\")\n",
        "        else:\n",
        "            self.ema = None\n",
        "\n",
        "    def get_train_op(self, loss: tf.Tensor, learning_rate: Union[tf.Tensor, float], **kwargs) -> tf.Operation:\n",
        "        assert \"learnable_scopes\" not in kwargs, \"learnable scopes unsupported\"\n",
        "        # train_op for bert variables\n",
        "        kwargs['learnable_scopes'] = ('bert/encoder', 'bert/embeddings')\n",
        "        if self.freeze_embeddings:\n",
        "            kwargs['learnable_scopes'] = ('bert/encoder',)\n",
        "        bert_learning_rate = learning_rate * self.bert_learning_rate_multiplier\n",
        "        bert_train_op = super().get_train_op(loss,\n",
        "                                             bert_learning_rate,\n",
        "                                             **kwargs)\n",
        "        # train_op for ner head variables\n",
        "        kwargs['learnable_scopes'] = ('ner',)\n",
        "        # print('1 get train op')\n",
        "        head_train_op = super().get_train_op(loss,\n",
        "                                             learning_rate,\n",
        "                                             **kwargs)\n",
        "        return tf.group(bert_train_op, head_train_op)\n",
        "\n",
        "    def _build_basic_feed_dict(self, input_ids: tf.Tensor, input_masks: tf.Tensor,\n",
        "                               token_types: Optional[tf.Tensor]=None, train: bool=False) -> dict:\n",
        "        \"\"\"Fills the feed_dict with the tensors defined in the basic class.\n",
        "        You need to update this dict by the values of output placeholders\n",
        "        and class-specific network inputs in your derived class.\n",
        "        \"\"\"\n",
        "        # print('1 _build_basic_feed_dict')\n",
        "        feed_dict = {\n",
        "            self.input_ids_ph: input_ids,\n",
        "            self.input_masks_ph: input_masks,\n",
        "        }\n",
        "        if token_types is not None:\n",
        "            feed_dict[self.token_types_ph] = token_types\n",
        "        if train:\n",
        "            # print('1 _build_basic_feed_dict train')\n",
        "            feed_dict.update({\n",
        "                self.learning_rate_ph: max(self.get_learning_rate(), self.min_learning_rate),\n",
        "                self.keep_prob_ph: self.keep_prob,\n",
        "                self.encoder_keep_prob_ph: 1.0 - self.encoder_dropout,\n",
        "                self.is_train_ph: True,\n",
        "            })\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def _build_feed_dict(self, input_ids, input_masks, token_types=None, *args,  **kwargs):\n",
        "        raise NotImplementedError(\"You must implement _build_feed_dict in your derived class.\")\n",
        "\n",
        "    def train_on_batch(self,\n",
        "                       input_ids: Union[List[List[int]], np.ndarray],\n",
        "                       input_masks: Union[List[List[int]], np.ndarray],\n",
        "                      #  topk_tok_mask: Union[List[List[int]], np.ndarray]\n",
        "                       *args, **kwargs) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: batch of indices of subwords\n",
        "            input_masks: batch of masks which determine what should be attended\n",
        "            args: arguments passed  to _build_feed_dict\n",
        "                and corresponding to additional input\n",
        "                and output tensors of the derived class.\n",
        "            kwargs: keyword arguments passed to _build_feed_dict\n",
        "                and corresponding to additional input\n",
        "                and output tensors of the derived class.\n",
        "\n",
        "        Returns:\n",
        "            dict with fields 'loss', 'head_learning_rate', and 'bert_learning_rate'\n",
        "        \"\"\"\n",
        "        # print('1 tr_on_batch1')\n",
        "        feed_dict = self._build_feed_dict(input_ids, input_masks, *args, **kwargs)\n",
        "        # print('1 feed_dict_tr_batch')\n",
        "        # print('1 tr_on_batch2')\n",
        "        if self.ema:\n",
        "            self.sess.run(self.ema.switch_to_train_op)\n",
        "        # print('1 sess with loss')\n",
        "        # print(self.topk_tok_mask)\n",
        "        _, loss, lr = self.sess.run([self.train_op, self.loss, self.learning_rate_ph],\n",
        "                                     feed_dict=feed_dict)\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'head_learning_rate': float(lr),\n",
        "                'bert_learning_rate': float(lr) * self.bert_learning_rate_multiplier}\n",
        "\n",
        "    def __call__(self,\n",
        "                 input_ids: Union[List[List[int]], np.ndarray],\n",
        "                 input_masks: Union[List[List[int]], np.ndarray],\n",
        "                 **kwargs) -> Union[List[List[int]], List[np.ndarray]]:\n",
        "        raise NotImplementedError(\"You must implement method __call__ in your derived class.\")\n",
        "\n",
        "    def save(self, exclude_scopes=('Optimizer', 'EMA/BackupVariables')) -> None:\n",
        "        if self.ema:\n",
        "            self.sess.run(self.ema.switch_to_train_op)\n",
        "        return super().save(exclude_scopes=exclude_scopes)\n",
        "\n",
        "    def load(self,\n",
        "             exclude_scopes=('Optimizer',\n",
        "                             'learning_rate',\n",
        "                             'momentum',\n",
        "                             'EMA/BackupVariables'),\n",
        "             **kwargs) -> None:\n",
        "        return super().load(exclude_scopes=exclude_scopes, **kwargs)\n",
        "\n",
        "\n",
        "@register('my_bert_sequence_tagger')\n",
        "class MyBertSequenceTagger(MyBertSequenceNetwork):\n",
        "    \"\"\"BERT-based model for text tagging. It predicts a label for every token (not subtoken) in the text.\n",
        "    You can use it for sequence labeling tasks, such as morphological tagging or named entity recognition.\n",
        "    See :class:`deeppavlov.models.bert.bert_sequence_tagger.BertSequenceNetwork`\n",
        "    for the description of inherited parameters.\n",
        "    Args:\n",
        "        n_tags: number of distinct tags\n",
        "        use_crf: whether to use CRF on top or not\n",
        "        use_birnn: whether to use bidirection rnn after BERT layers.\n",
        "            For NER and morphological tagging we usually set it to `False` as otherwise the model overfits\n",
        "        birnn_cell_type: the type of Bidirectional RNN. Either `lstm` or `gru`\n",
        "        birnn_hidden_size: number of hidden units in the BiRNN layer in each direction\n",
        "        return_probas: set this to `True` if you need the probabilities instead of raw answers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_tags: List[str],\n",
        "                 keep_prob: float,\n",
        "                 bert_config_file: str,\n",
        "                 pretrained_bert: str = None,\n",
        "                 attention_probs_keep_prob: float = None,\n",
        "                 hidden_keep_prob: float = None,\n",
        "                 use_crf=False,\n",
        "                 encoder_layer_ids: List[int] = (-1,),\n",
        "                 encoder_dropout: float = 0.0,\n",
        "                 optimizer: str = None,\n",
        "                 weight_decay_rate: float = 1e-6,\n",
        "                 use_birnn: bool = False,\n",
        "                 birnn_cell_type: str = 'lstm',\n",
        "                 birnn_hidden_size: int = 128,\n",
        "                 ema_decay: float = None,\n",
        "                 ema_variables_on_cpu: bool = True,\n",
        "                 return_probas: bool = False,\n",
        "                 freeze_embeddings: bool = False,\n",
        "                 learning_rate: float = 1e-3,\n",
        "                 bert_learning_rate: float = 2e-5,\n",
        "                 min_learning_rate: float = 1e-07,\n",
        "                 learning_rate_drop_patience: int = 20,\n",
        "                 learning_rate_drop_div: float = 2.0,\n",
        "                 load_before_drop: bool = True,\n",
        "                 clip_norm: float = 1.0,\n",
        "                 **kwargs) -> None:\n",
        "        self.n_tags = n_tags\n",
        "        self.use_crf = use_crf\n",
        "        self.use_birnn = use_birnn\n",
        "        self.birnn_cell_type = birnn_cell_type\n",
        "        self.birnn_hidden_size = birnn_hidden_size\n",
        "        self.return_probas = return_probas\n",
        "        super().__init__(keep_prob=keep_prob,\n",
        "                         bert_config_file=bert_config_file,\n",
        "                         pretrained_bert=pretrained_bert,\n",
        "                         attention_probs_keep_prob=attention_probs_keep_prob,\n",
        "                         hidden_keep_prob=hidden_keep_prob,\n",
        "                         encoder_layer_ids=encoder_layer_ids,\n",
        "                         encoder_dropout=encoder_dropout,\n",
        "                         optimizer=optimizer,\n",
        "                         weight_decay_rate=weight_decay_rate,\n",
        "                         ema_decay=ema_decay,\n",
        "                         ema_variables_on_cpu=ema_variables_on_cpu,\n",
        "                         freeze_embeddings=freeze_embeddings,\n",
        "                         learning_rate=learning_rate,\n",
        "                         bert_learning_rate=bert_learning_rate,\n",
        "                         min_learning_rate=min_learning_rate,\n",
        "                         learning_rate_drop_div=learning_rate_drop_div,\n",
        "                         learning_rate_drop_patience=learning_rate_drop_patience,\n",
        "                         load_before_drop=load_before_drop,\n",
        "                         clip_norm=clip_norm,\n",
        "                         **kwargs)\n",
        "\n",
        "    def _init_graph(self) -> None:\n",
        "        self._init_placeholders()\n",
        "\n",
        "        units = super()._init_graph()\n",
        "\n",
        "        with tf.variable_scope('ner'):\n",
        "            if self.use_birnn:\n",
        "                units, _ = bi_rnn(units,\n",
        "                                  self.birnn_hidden_size,\n",
        "                                  cell_type=self.birnn_cell_type,\n",
        "                                  seq_lengths=self.seq_lengths,\n",
        "                                  name='birnn')\n",
        "                units = tf.concat(units, -1)\n",
        "            # TODO: maybe add one more layer?\n",
        "            logits = tf.layers.dense(units, units=self.n_tags, name=\"output_dense\")\n",
        "            self.logits = token_from_subtoken(logits, self.y_masks_ph) #token_ids of startwords?\n",
        "\n",
        "            # CRF\n",
        "            if self.use_crf:\n",
        "                transition_params = tf.get_variable('Transition_Params',\n",
        "                                                    shape=[self.n_tags, self.n_tags],\n",
        "                                                    initializer=tf.zeros_initializer())\n",
        "                log_likelihood, transition_params = \\\n",
        "                    tf.contrib.crf.crf_log_likelihood(self.logits,\n",
        "                                                      self.y_ph,\n",
        "                                                      self.seq_lengths,\n",
        "                                                      transition_params)\n",
        "                loss_tensor = -log_likelihood\n",
        "                self._transition_params = transition_params\n",
        "\n",
        "            self.y_predictions = tf.argmax(self.logits, -1)\n",
        "            self.y_probas = tf.nn.softmax(self.logits, axis=2)\n",
        "\n",
        "        with tf.variable_scope(\"loss\"):\n",
        "            tag_mask = self._get_tag_mask()\n",
        "            y_mask = tf.cast(tag_mask, tf.float32)\n",
        "            if self.use_crf:\n",
        "                self.loss = tf.reduce_mean(loss_tensor)\n",
        "            else:                \n",
        "                self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.y_ph,\n",
        "                                                                   logits=self.logits,\n",
        "                                                                   weights=self.topk_masks_ph)  #instead of y_mask\n",
        "\n",
        "\n",
        "    def _init_placeholders(self) -> None:\n",
        "        super()._init_placeholders()\n",
        "        self.y_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='y_ph')\n",
        "        self.y_masks_ph = tf.placeholder(shape=(None, None),\n",
        "                                         dtype=tf.int32,\n",
        "                                         name='y_mask_ph')\n",
        "        self.topk_masks_ph = tf.placeholder(shape=(None, None),  # added\n",
        "                                         dtype=tf.int32,\n",
        "                                         name='topk_mask_ph')\n",
        "\n",
        "    def _decode_crf(self, feed_dict: Dict[tf.Tensor, np.ndarray]) -> List[np.ndarray]:\n",
        "        logits, trans_params, mask, seq_lengths = self.sess.run([self.logits,\n",
        "                                                                 self._transition_params,\n",
        "                                                                 self.y_masks_ph,\n",
        "                                                                 self.seq_lengths],\n",
        "                                                                feed_dict=feed_dict)\n",
        "        # iterate over the sentences because no batching in viterbi_decode\n",
        "        y_pred = []\n",
        "        for logit, sequence_length in zip(logits, seq_lengths):\n",
        "            logit = logit[:int(sequence_length)]  # keep only the valid steps\n",
        "            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n",
        "            y_pred += [viterbi_seq]\n",
        "        return y_pred\n",
        "\n",
        "    def _build_feed_dict(self, input_ids, input_masks, y_masks, topk_tok_mask, y=None):\n",
        "        feed_dict = self._build_basic_feed_dict(input_ids, input_masks, train=(y is not None))\n",
        "\n",
        "        if y is not None:\n",
        "            max_len = 0\n",
        "            for seq in topk_tok_mask:\n",
        "                if len(seq) > max_len:\n",
        "                    max_len = len(seq)\n",
        "            feed_dict[self.topk_masks_ph] = topk_tok_mask # added            \n",
        "            feed_dict[self.y_ph] = y\n",
        "        feed_dict[self.y_masks_ph] = y_masks\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def __call__(self,\n",
        "                 input_ids: Union[List[List[int]], np.ndarray],\n",
        "                 input_masks: Union[List[List[int]], np.ndarray],\n",
        "                 y_masks: Union[List[List[int]], np.ndarray],\n",
        "                 topk_tok_mask: Union[List[List[int]], np.ndarray]) -> Union[List[List[int]], List[np.ndarray]]:\n",
        "        \"\"\" Predicts tag indices for a given subword tokens batch\n",
        "        Args:\n",
        "            input_ids: indices of the subwords\n",
        "            input_masks: mask that determines where to attend and where not to\n",
        "            y_masks: mask which determines the first subword units in the the word\n",
        "\n",
        "        Added: \n",
        "            topk_tok_mask: mask that determines where to attend and where not to\n",
        "        Returns:\n",
        "            Label indices or class probabilities for each token (not subtoken)\n",
        "        # \"\"\"\n",
        "        \n",
        "        feed_dict = self._build_feed_dict(input_ids, input_masks, y_masks, topk_tok_mask)\n",
        "       \n",
        "        if self.ema:\n",
        "            self.sess.run(self.ema.switch_to_test_op)\n",
        "        if not self.return_probas:\n",
        "            if self.use_crf:\n",
        "                pred = self._decode_crf(feed_dict)\n",
        "            else:\n",
        "                pred, seq_lengths = self.sess.run([self.y_predictions, self.seq_lengths], feed_dict=feed_dict)\n",
        "                pred = [p[:l] for l, p in zip(seq_lengths, pred)]\n",
        "        else:\n",
        "            pred = self.sess.run(self.y_probas, feed_dict=feed_dict)\n",
        "\n",
        "        return pred\n",
        "\n",
        "\n",
        "class ExponentialMovingAverage:\n",
        "    def __init__(self,\n",
        "                 decay: float = 0.999,\n",
        "                 variables_on_cpu: bool = True) -> None:\n",
        "        self.decay = decay\n",
        "        self.ema = tf.train.ExponentialMovingAverage(decay=decay)\n",
        "        self.var_device_name = '/cpu:0' if variables_on_cpu else None\n",
        "        self.train_mode = None\n",
        "\n",
        "    def build(self,\n",
        "              minimize_op: tf.Tensor,\n",
        "              update_vars: List[tf.Variable] = None,\n",
        "              name: str = \"EMA\") -> tf.Tensor:\n",
        "        with tf.variable_scope(name):\n",
        "            if update_vars is None:\n",
        "                update_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "            with tf.control_dependencies([minimize_op]):\n",
        "                minimize_op = self.ema.apply(update_vars)\n",
        "\n",
        "            with tf.device(self.var_device_name):\n",
        "                # Make backup variables\n",
        "                with tf.variable_scope('BackupVariables'):\n",
        "                    backup_vars = [tf.get_variable(var.op.name,\n",
        "                                                   dtype=var.value().dtype,\n",
        "                                                   trainable=False,\n",
        "                                                   initializer=var.initialized_value())\n",
        "                                   for var in update_vars]\n",
        "\n",
        "                def ema_to_weights():\n",
        "                    return tf.group(*(tf.assign(var, self.ema.average(var).read_value())\n",
        "                                      for var in update_vars))\n",
        "\n",
        "                def save_weight_backups():\n",
        "                    return tf.group(*(tf.assign(bck, var.read_value())\n",
        "                                      for var, bck in zip(update_vars, backup_vars)))\n",
        "\n",
        "                def restore_weight_backups():\n",
        "                    return tf.group(*(tf.assign(var, bck.read_value())\n",
        "                                      for var, bck in zip(update_vars, backup_vars)))\n",
        "\n",
        "                train_switch_op = restore_weight_backups()\n",
        "                with tf.control_dependencies([save_weight_backups()]):\n",
        "                    test_switch_op = ema_to_weights()\n",
        "\n",
        "            self.train_switch_op = train_switch_op\n",
        "            self.test_switch_op = test_switch_op\n",
        "            self.do_nothing_op = tf.no_op()\n",
        "\n",
        "        return minimize_op\n",
        "\n",
        "    @property\n",
        "    def init_op(self) -> tf.Operation:\n",
        "        self.train_mode = False\n",
        "        return self.test_switch_op\n",
        "\n",
        "    @property\n",
        "    def switch_to_train_op(self) -> tf.Operation:\n",
        "        assert self.train_mode is not None, \"ema variables aren't initialized\"\n",
        "        if not self.train_mode:\n",
        "            # log.info(\"switching to train mode\")\n",
        "            self.train_mode = True\n",
        "            return self.train_switch_op\n",
        "        return self.do_nothing_op\n",
        "\n",
        "    @property\n",
        "    def switch_to_test_op(self) -> tf.Operation:\n",
        "        assert self.train_mode is not None, \"ema variables aren't initialized\"\n",
        "        if self.train_mode:\n",
        "            # log.info(\"switching to test mode\")\n",
        "            self.train_mode = False\n",
        "            return self.test_switch_op\n",
        "        return self.do_nothing_op"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrMdYc1dG7eO"
      },
      "source": [
        "## Mask_ys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Component that masks tokens for the evaluation, so the metrics are counted considering certain group of tokens. By masking here I mean just forming new lists of tokens without the irrelevant ones."
      ],
      "metadata": {
        "id": "_yquqwCeE63b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oS5h0kWtGx2e"
      },
      "outputs": [],
      "source": [
        "@register('mask_ys')\n",
        "class mask_ys(Component):\n",
        "    def __init__(self, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, y, topk_tok_mask, y_predicted, *args, **kwargs):\n",
        "\n",
        "        y_predicted_masked = []\n",
        "        y_masked = []\n",
        "        \n",
        "        for i in range(len(y_predicted)):\n",
        "            line_preds = []\n",
        "            line_true = []\n",
        "            for j in range(len(y_predicted[i])):\n",
        "                if topk_tok_mask[i][j] == 1:\n",
        "                    line_preds.append(y_predicted[i][j])\n",
        "                    line_true.append(y[i][j])\n",
        "            y_predicted_masked.append(np.array(line_preds))\n",
        "            y_masked.append(np.array(line_true))\n",
        "\n",
        "        return y_masked, y_predicted_masked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XSodQDbIMDo"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to specify in config:\n",
        "- ['dataset_reader']['data_path'] - path to datafolder\n",
        "- ['dataset_reader']['language'] - language according to deeppavlov's config \n",
        "- ['metadata']['variables']['MODELS_PATH'] - path to general folder devoted to certain dataset\n",
        "- ['train']['evaluation_targets'] = valid, ['dataset_reader']['data_types'] = ['train', 'dev'], if there is no test file\n",
        "- ['metadata']['variables']['DATA_PATH'] - path to datafolder\n",
        "\n",
        "will be specified below:\n",
        "- ['metadata']['variables']['WORK_PATH'] - path to certain model folder\n",
        "- ['chainer']['pipe'][1]['topk_tokens_path'] - path to the group of tokens to consider\n"
      ],
      "metadata": {
        "id": "yYptObzp3DO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d-rIu6PaIb6J"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import json\n",
        "from deeppavlov import build_model, configs, train_model, evaluate_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TUJPGa0qMu86"
      },
      "outputs": [],
      "source": [
        "base_config = json.load(open('./base_config.json'))\n",
        "\n",
        "base_config['metadata']['variables']['DATA_PATH'] = datafolder_path\n",
        "base_config['metadata']['variables']['MODELS_PATH'] = '{ROOT_PATH}/drive/MyDrive/models_diploma/GramEval_poetry'\n",
        "base_config['train']['evaluation_targets'] = ['valid']\n",
        "base_config['dataset_reader']['data_types'] = ['train', 'dev']\n",
        "base_config['dataset_reader']['language'] = language_conf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##From Scratch Models"
      ],
      "metadata": {
        "id": "_6HdtWbaGTiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###top100 "
      ],
      "metadata": {
        "id": "msN4Q1KmT5Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dir for my models\n",
        "base_config['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/fs_models/model_top100'\n",
        "base_config['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt']\n",
        "base_config['train']['epochs'] = 60\n",
        "base_config['train']['validation_patience'] = 30\n",
        "\n",
        "base_config['chainer']['pipe'][1]['last'] = False"
      ],
      "metadata": {
        "id": "oAPZjNClGYkm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(base_config, download=False) \n"
      ],
      "metadata": {
        "id": "1nwPq2KxGefG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cde961-86da-41a8-dcf6-bc820acef090"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:49:41.712 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:150: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:49:42.123 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From <ipython-input-6-5405f03ac0b9>:54: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-5405f03ac0b9>:228: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/downloads/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:50:01.545 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 16:50:03.501 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.0, \"accuracy\": 0.1029}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:178: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:50:14.246 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.0 to 0.4362\n",
            "2022-05-23 16:50:14.247 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:50:14.253 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.4362, \"accuracy\": 0.1765}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 0, \"patience_limit\": 30}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:211: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:50:26.456 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.4362 to 0.5101\n",
            "2022-05-23 16:50:26.457 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:50:26.468 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.5101, \"accuracy\": 0.2059}, \"time_spent\": \"0:00:25\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:50:38.623 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.5101 to 0.604\n",
            "2022-05-23 16:50:38.628 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:50:38.632 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.604, \"accuracy\": 0.2941}, \"time_spent\": \"0:00:38\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:50:50.539 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.604 to 0.6779\n",
            "2022-05-23 16:50:50.543 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:50:50.551 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.6779, \"accuracy\": 0.3382}, \"time_spent\": \"0:00:49\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:51:03.872 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.6779 to 0.7215\n",
            "2022-05-23 16:51:03.874 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:51:03.883 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7215, \"accuracy\": 0.3971}, \"time_spent\": \"0:01:03\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:51:16.792 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7215 to 0.7752\n",
            "2022-05-23 16:51:16.793 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:51:16.800 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7752, \"accuracy\": 0.4559}, \"time_spent\": \"0:01:16\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:51:29.784 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7752 to 0.7919\n",
            "2022-05-23 16:51:29.785 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:51:29.792 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7919, \"accuracy\": 0.4706}, \"time_spent\": \"0:01:29\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:51:43.11 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7919 to 0.802\n",
            "2022-05-23 16:51:43.13 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:51:43.22 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.802, \"accuracy\": 0.5}, \"time_spent\": \"0:01:42\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:51:55.640 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.802 to 0.8154\n",
            "2022-05-23 16:51:55.642 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:51:55.652 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8154, \"accuracy\": 0.5147}, \"time_spent\": \"0:01:55\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:52:09.326 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8154 to 0.8221\n",
            "2022-05-23 16:52:09.327 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:52:09.340 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8221, \"accuracy\": 0.5441}, \"time_spent\": \"0:02:08\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:52:18.212 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8188, \"accuracy\": 0.5294}, \"time_spent\": \"0:02:17\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9457, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:52:21.980 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8221 to 0.8456\n",
            "2022-05-23 16:52:21.986 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:52:21.993 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8456, \"accuracy\": 0.5882}, \"time_spent\": \"0:02:21\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:52:35.181 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8356, \"accuracy\": 0.5735}, \"time_spent\": \"0:02:34\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:52:43.727 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8456 to 0.8557\n",
            "2022-05-23 16:52:43.729 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:52:43.736 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8557, \"accuracy\": 0.6029}, \"time_spent\": \"0:02:43\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:52:56.818 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8557 to 0.8893\n",
            "2022-05-23 16:52:56.819 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:52:56.830 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8893, \"accuracy\": 0.6471}, \"time_spent\": \"0:02:56\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:53:10.231 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8859, \"accuracy\": 0.6618}, \"time_spent\": \"0:03:09\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:53:18.337 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8893 to 0.8993\n",
            "2022-05-23 16:53:18.341 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:53:18.348 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8993, \"accuracy\": 0.7206}, \"time_spent\": \"0:03:17\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:53:30.995 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8792, \"accuracy\": 0.6324}, \"time_spent\": \"0:03:30\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:53:39.277 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8993 to 0.9094\n",
            "2022-05-23 16:53:39.279 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:53:39.287 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9094, \"accuracy\": 0.7353}, \"time_spent\": \"0:03:38\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:53:52.154 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9094 to 0.9161\n",
            "2022-05-23 16:53:52.157 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:53:52.163 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9161, \"accuracy\": 0.75}, \"time_spent\": \"0:03:51\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:54:05.180 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9161, \"accuracy\": 0.7353}, \"time_spent\": \"0:04:04\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:54:11.359 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9161 to 0.9228\n",
            "2022-05-23 16:54:11.361 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:54:11.371 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9228, \"accuracy\": 0.75}, \"time_spent\": \"0:04:10\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18927, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:54:18.813 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9195, \"accuracy\": 0.75}, \"time_spent\": \"0:04:18\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:54:27.41 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9228 to 0.9262\n",
            "2022-05-23 16:54:27.43 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:54:27.54 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9262, \"accuracy\": 0.7647}, \"time_spent\": \"0:04:26\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:54:41.0 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9262 to 0.9295\n",
            "2022-05-23 16:54:41.6 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:54:41.15 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9295, \"accuracy\": 0.7647}, \"time_spent\": \"0:04:40\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:54:54.550 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9295 to 0.9497\n",
            "2022-05-23 16:54:54.551 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:54:54.562 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9497, \"accuracy\": 0.8235}, \"time_spent\": \"0:04:53\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:55:08.239 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.943, \"accuracy\": 0.8235}, \"time_spent\": \"0:05:07\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:55:16.527 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.943, \"accuracy\": 0.7941}, \"time_spent\": \"0:05:15\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:55:24.796 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9497 to 0.953\n",
            "2022-05-23 16:55:24.797 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:55:24.805 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.953, \"accuracy\": 0.8235}, \"time_spent\": \"0:05:24\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:55:37.381 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.953 to 0.9564\n",
            "2022-05-23 16:55:37.382 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:55:37.388 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9564, \"accuracy\": 0.8235}, \"time_spent\": \"0:05:36\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:55:51.774 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9564, \"accuracy\": 0.8235}, \"time_spent\": \"0:05:51\", \"epochs_done\": 29, \"batches_seen\": 841, \"train_examples_seen\": 26535, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:56:00.199 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9564 to 0.9597\n",
            "2022-05-23 16:56:00.201 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:56:00.209 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9597, \"accuracy\": 0.8382}, \"time_spent\": \"0:05:59\", \"epochs_done\": 30, \"batches_seen\": 870, \"train_examples_seen\": 27450, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:56:13.611 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9597 to 0.9631\n",
            "2022-05-23 16:56:13.612 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:56:13.630 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9631, \"accuracy\": 0.8529}, \"time_spent\": \"0:06:13\", \"epochs_done\": 31, \"batches_seen\": 899, \"train_examples_seen\": 28365, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:56:20.629 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9631, \"accuracy\": 0.8529}, \"time_spent\": \"0:06:20\", \"epochs_done\": 31, \"batches_seen\": 900, \"train_examples_seen\": 28397, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:56:28.42 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.953, \"accuracy\": 0.8235}, \"time_spent\": \"0:06:27\", \"epochs_done\": 32, \"batches_seen\": 928, \"train_examples_seen\": 29280, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:56:36.759 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9597, \"accuracy\": 0.8235}, \"time_spent\": \"0:06:36\", \"epochs_done\": 33, \"batches_seen\": 957, \"train_examples_seen\": 30195, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:56:44.961 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9631 to 0.9664\n",
            "2022-05-23 16:56:44.962 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:56:44.971 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9664, \"accuracy\": 0.8529}, \"time_spent\": \"0:06:44\", \"epochs_done\": 34, \"batches_seen\": 986, \"train_examples_seen\": 31110, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:56:57.965 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9664, \"accuracy\": 0.8529}, \"time_spent\": \"0:06:57\", \"epochs_done\": 35, \"batches_seen\": 1015, \"train_examples_seen\": 32025, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:57:06.505 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9664 to 0.9698\n",
            "2022-05-23 16:57:06.509 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:57:06.514 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9698, \"accuracy\": 0.8676}, \"time_spent\": \"0:07:05\", \"epochs_done\": 36, \"batches_seen\": 1044, \"train_examples_seen\": 32940, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:57:19.325 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9698 to 0.9732\n",
            "2022-05-23 16:57:19.329 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:57:19.335 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9732, \"accuracy\": 0.8824}, \"time_spent\": \"0:07:18\", \"epochs_done\": 37, \"batches_seen\": 1073, \"train_examples_seen\": 33855, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:57:32.506 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9732, \"accuracy\": 0.8824}, \"time_spent\": \"0:07:31\", \"epochs_done\": 38, \"batches_seen\": 1102, \"train_examples_seen\": 34770, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:57:40.952 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9698, \"accuracy\": 0.8824}, \"time_spent\": \"0:07:40\", \"epochs_done\": 39, \"batches_seen\": 1131, \"train_examples_seen\": 35685, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:57:49.365 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9732, \"accuracy\": 0.8824}, \"time_spent\": \"0:07:48\", \"epochs_done\": 40, \"batches_seen\": 1160, \"train_examples_seen\": 36600, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:57:58.41 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9732 to 0.9799\n",
            "2022-05-23 16:57:58.42 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:57:58.51 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9799, \"accuracy\": 0.9118}, \"time_spent\": \"0:07:57\", \"epochs_done\": 41, \"batches_seen\": 1189, \"train_examples_seen\": 37515, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:58:07.943 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9799, \"accuracy\": 0.9118}, \"time_spent\": \"0:08:07\", \"epochs_done\": 41, \"batches_seen\": 1200, \"train_examples_seen\": 37854, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:58:12.391 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9799 to 0.9832\n",
            "2022-05-23 16:58:12.392 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:58:12.405 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9832, \"accuracy\": 0.9265}, \"time_spent\": \"0:08:11\", \"epochs_done\": 42, \"batches_seen\": 1218, \"train_examples_seen\": 38430, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:58:26.394 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9832, \"accuracy\": 0.9265}, \"time_spent\": \"0:08:25\", \"epochs_done\": 43, \"batches_seen\": 1247, \"train_examples_seen\": 39345, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:58:34.862 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9799, \"accuracy\": 0.9118}, \"time_spent\": \"0:08:34\", \"epochs_done\": 44, \"batches_seen\": 1276, \"train_examples_seen\": 40260, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:58:43.301 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9832, \"accuracy\": 0.9265}, \"time_spent\": \"0:08:42\", \"epochs_done\": 45, \"batches_seen\": 1305, \"train_examples_seen\": 41175, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:58:51.617 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9799, \"accuracy\": 0.9118}, \"time_spent\": \"0:08:51\", \"epochs_done\": 46, \"batches_seen\": 1334, \"train_examples_seen\": 42090, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:58:59.970 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9832 to 0.9866\n",
            "2022-05-23 16:58:59.971 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:58:59.979 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9866, \"accuracy\": 0.9412}, \"time_spent\": \"0:08:59\", \"epochs_done\": 47, \"batches_seen\": 1363, \"train_examples_seen\": 43005, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:59:13.134 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9832, \"accuracy\": 0.9265}, \"time_spent\": \"0:09:12\", \"epochs_done\": 48, \"batches_seen\": 1392, \"train_examples_seen\": 43920, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:59:22.300 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9866, \"accuracy\": 0.9412}, \"time_spent\": \"0:09:21\", \"epochs_done\": 49, \"batches_seen\": 1421, \"train_examples_seen\": 44835, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:59:30.711 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9866, \"accuracy\": 0.9412}, \"time_spent\": \"0:09:30\", \"epochs_done\": 50, \"batches_seen\": 1450, \"train_examples_seen\": 45750, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:59:39.154 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9866, \"accuracy\": 0.9412}, \"time_spent\": \"0:09:38\", \"epochs_done\": 51, \"batches_seen\": 1479, \"train_examples_seen\": 46665, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:59:46.115 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9866 to 0.9899\n",
            "2022-05-23 16:59:46.116 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 16:59:46.124 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9899, \"accuracy\": 0.9559}, \"time_spent\": \"0:09:45\", \"epochs_done\": 51, \"batches_seen\": 1500, \"train_examples_seen\": 47324, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 16:59:52.482 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9866, \"accuracy\": 0.9412}, \"time_spent\": \"0:09:51\", \"epochs_done\": 52, \"batches_seen\": 1508, \"train_examples_seen\": 47580, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:00:00.937 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9832, \"accuracy\": 0.9265}, \"time_spent\": \"0:10:00\", \"epochs_done\": 53, \"batches_seen\": 1537, \"train_examples_seen\": 48495, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:00:09.894 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9899, \"accuracy\": 0.9559}, \"time_spent\": \"0:10:09\", \"epochs_done\": 54, \"batches_seen\": 1566, \"train_examples_seen\": 49410, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:00:18.278 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9899 to 0.9933\n",
            "2022-05-23 17:00:18.280 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:00:18.289 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:10:17\", \"epochs_done\": 55, \"batches_seen\": 1595, \"train_examples_seen\": 50325, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:00:31.571 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9899, \"accuracy\": 0.9559}, \"time_spent\": \"0:10:31\", \"epochs_done\": 56, \"batches_seen\": 1624, \"train_examples_seen\": 51240, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:00:40.511 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9866, \"accuracy\": 0.9412}, \"time_spent\": \"0:10:39\", \"epochs_done\": 57, \"batches_seen\": 1653, \"train_examples_seen\": 52155, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:00:48.908 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:10:48\", \"epochs_done\": 58, \"batches_seen\": 1682, \"train_examples_seen\": 53070, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:00:57.389 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:10:56\", \"epochs_done\": 59, \"batches_seen\": 1711, \"train_examples_seen\": 53985, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:01:05.968 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:11:05\", \"epochs_done\": 60, \"batches_seen\": 1740, \"train_examples_seen\": 54900, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:01:07.563 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/tag.dict]\n",
            "2022-05-23 17:01:21.673 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:01:23.508 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:01:25.25 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/tag.dict]\n",
            "2022-05-23 17:01:42.32 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top100/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:01:43.831 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###top1000"
      ],
      "metadata": {
        "id": "DavKW8gHT1vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dir for my models\n",
        "base_config['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/fs_models/model_top1000'\n",
        "base_config['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/1.txt']\n",
        "base_config['train']['validation_patience'] = 30\n",
        "base_config['train']['epochs'] = 60\n",
        "\n",
        "base_config['chainer']['pipe'][1]['last'] = False"
      ],
      "metadata": {
        "id": "X7pxTy9oT97h"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(base_config, download=False) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2VKaif8UFps",
        "outputId": "23962e39-6ec3-4de8-a2f4-a889f7eec06c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:02:20.585 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
            "2022-05-23 17:02:20.983 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/downloads/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:02:42.298 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 17:02:43.493 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.0, \"accuracy\": 0.1029}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:02:54.850 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.0 to 0.0748\n",
            "2022-05-23 17:02:54.851 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:02:54.860 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.0748, \"accuracy\": 0.1618}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:03:07.523 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.0748 to 0.1168\n",
            "2022-05-23 17:03:07.524 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:03:07.532 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.1168, \"accuracy\": 0.1912}, \"time_spent\": \"0:00:26\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:03:20.907 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.1168 to 0.1355\n",
            "2022-05-23 17:03:20.908 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:03:20.917 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.1355, \"accuracy\": 0.1765}, \"time_spent\": \"0:00:39\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:03:33.341 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.1355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.1355, \"accuracy\": 0.1324}, \"time_spent\": \"0:00:52\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:03:41.998 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.1355 to 0.1729\n",
            "2022-05-23 17:03:42.0 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:03:42.14 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.1729, \"accuracy\": 0.1324}, \"time_spent\": \"0:01:00\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:03:54.114 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.1729 to 0.2757\n",
            "2022-05-23 17:03:54.116 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:03:54.128 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.2757, \"accuracy\": 0.2353}, \"time_spent\": \"0:01:12\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:04:08.815 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.2757 to 0.3692\n",
            "2022-05-23 17:04:08.816 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:04:08.824 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.3692, \"accuracy\": 0.2794}, \"time_spent\": \"0:01:27\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:04:21.897 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.3692 to 0.486\n",
            "2022-05-23 17:04:21.898 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:04:21.906 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.486, \"accuracy\": 0.3676}, \"time_spent\": \"0:01:40\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:04:35.269 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.486 to 0.5654\n",
            "2022-05-23 17:04:35.271 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:04:35.286 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.5654, \"accuracy\": 0.4118}, \"time_spent\": \"0:01:53\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:04:48.827 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.5654 to 0.6215\n",
            "2022-05-23 17:04:48.828 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:04:48.836 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.6215, \"accuracy\": 0.4412}, \"time_spent\": \"0:02:07\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:04:56.778 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.6215 to 0.6449\n",
            "2022-05-23 17:04:56.779 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:04:56.787 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.6449, \"accuracy\": 0.4853}, \"time_spent\": \"0:02:15\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9470, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:05:07.184 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.6449 to 0.7336\n",
            "2022-05-23 17:05:07.189 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:05:07.194 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7336, \"accuracy\": 0.5882}, \"time_spent\": \"0:02:25\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:05:20.924 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7336 to 0.771\n",
            "2022-05-23 17:05:20.927 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:05:20.933 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.771, \"accuracy\": 0.6029}, \"time_spent\": \"0:02:39\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:05:34.584 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.771 to 0.7757\n",
            "2022-05-23 17:05:34.586 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:05:34.599 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7757, \"accuracy\": 0.6176}, \"time_spent\": \"0:02:53\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:05:49.155 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.7757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7664, \"accuracy\": 0.6324}, \"time_spent\": \"0:03:07\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:05:57.646 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7757 to 0.8224\n",
            "2022-05-23 17:05:57.651 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:05:57.657 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8224, \"accuracy\": 0.6618}, \"time_spent\": \"0:03:16\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:06:10.277 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8037, \"accuracy\": 0.6471}, \"time_spent\": \"0:03:28\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:06:18.723 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8224 to 0.8505\n",
            "2022-05-23 17:06:18.728 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:06:18.738 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8505, \"accuracy\": 0.7206}, \"time_spent\": \"0:03:37\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:06:32.122 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8505 to 0.8551\n",
            "2022-05-23 17:06:32.123 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:06:32.131 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8551, \"accuracy\": 0.7353}, \"time_spent\": \"0:03:50\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:06:46.802 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8551 to 0.8645\n",
            "2022-05-23 17:06:46.804 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:06:46.814 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8645, \"accuracy\": 0.75}, \"time_spent\": \"0:04:05\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:07:01.256 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8645 to 0.8832\n",
            "2022-05-23 17:07:01.258 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:07:01.271 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8832, \"accuracy\": 0.75}, \"time_spent\": \"0:04:19\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:07:11.826 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8785, \"accuracy\": 0.75}, \"time_spent\": \"0:04:30\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18940, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:07:14.716 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8785, \"accuracy\": 0.7353}, \"time_spent\": \"0:04:33\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:07:23.353 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8832 to 0.8879\n",
            "2022-05-23 17:07:23.356 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:07:23.367 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8879, \"accuracy\": 0.75}, \"time_spent\": \"0:04:42\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:07:37.154 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8879 to 0.9019\n",
            "2022-05-23 17:07:37.155 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:07:37.165 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9019, \"accuracy\": 0.75}, \"time_spent\": \"0:04:55\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:07:51.774 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9019 to 0.9112\n",
            "2022-05-23 17:07:51.776 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:07:51.785 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9112, \"accuracy\": 0.7647}, \"time_spent\": \"0:05:10\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:08:06.84 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9112 to 0.9299\n",
            "2022-05-23 17:08:06.85 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:08:06.94 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9299, \"accuracy\": 0.8088}, \"time_spent\": \"0:05:24\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:08:19.911 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9299, \"accuracy\": 0.8235}, \"time_spent\": \"0:05:38\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:08:28.712 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9299 to 0.9393\n",
            "2022-05-23 17:08:28.714 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:08:28.722 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9393, \"accuracy\": 0.8529}, \"time_spent\": \"0:05:47\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:08:41.701 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9393, \"accuracy\": 0.8382}, \"time_spent\": \"0:06:00\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:08:50.578 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9393 to 0.9486\n",
            "2022-05-23 17:08:50.579 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:08:50.587 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9486, \"accuracy\": 0.8529}, \"time_spent\": \"0:06:09\", \"epochs_done\": 29, \"batches_seen\": 841, \"train_examples_seen\": 26535, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:09:03.724 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9439, \"accuracy\": 0.8676}, \"time_spent\": \"0:06:22\", \"epochs_done\": 30, \"batches_seen\": 870, \"train_examples_seen\": 27450, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:09:12.599 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9486 to 0.9579\n",
            "2022-05-23 17:09:12.601 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:09:12.608 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9579, \"accuracy\": 0.8971}, \"time_spent\": \"0:06:31\", \"epochs_done\": 31, \"batches_seen\": 899, \"train_examples_seen\": 28365, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:09:18.518 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9579, \"accuracy\": 0.8971}, \"time_spent\": \"0:06:37\", \"epochs_done\": 31, \"batches_seen\": 900, \"train_examples_seen\": 28397, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:09:25.949 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9439, \"accuracy\": 0.8529}, \"time_spent\": \"0:06:44\", \"epochs_done\": 32, \"batches_seen\": 928, \"train_examples_seen\": 29280, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:09:34.874 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9486, \"accuracy\": 0.8676}, \"time_spent\": \"0:06:53\", \"epochs_done\": 33, \"batches_seen\": 957, \"train_examples_seen\": 30195, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:09:43.569 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9486, \"accuracy\": 0.8676}, \"time_spent\": \"0:07:02\", \"epochs_done\": 34, \"batches_seen\": 986, \"train_examples_seen\": 31110, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:09:52.109 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9533, \"accuracy\": 0.8824}, \"time_spent\": \"0:07:10\", \"epochs_done\": 35, \"batches_seen\": 1015, \"train_examples_seen\": 32025, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:10:00.636 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9579, \"accuracy\": 0.8971}, \"time_spent\": \"0:07:19\", \"epochs_done\": 36, \"batches_seen\": 1044, \"train_examples_seen\": 32940, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:10:09.128 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9579 to 0.9673\n",
            "2022-05-23 17:10:09.131 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:10:09.138 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9673, \"accuracy\": 0.9118}, \"time_spent\": \"0:07:27\", \"epochs_done\": 37, \"batches_seen\": 1073, \"train_examples_seen\": 33855, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:10:23.7 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9626, \"accuracy\": 0.8971}, \"time_spent\": \"0:07:41\", \"epochs_done\": 38, \"batches_seen\": 1102, \"train_examples_seen\": 34770, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:10:32.67 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9673 to 0.9766\n",
            "2022-05-23 17:10:32.71 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:10:32.78 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9766, \"accuracy\": 0.9265}, \"time_spent\": \"0:07:50\", \"epochs_done\": 39, \"batches_seen\": 1131, \"train_examples_seen\": 35685, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:10:45.448 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9766, \"accuracy\": 0.9265}, \"time_spent\": \"0:08:04\", \"epochs_done\": 40, \"batches_seen\": 1160, \"train_examples_seen\": 36600, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:10:54.193 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9766, \"accuracy\": 0.9265}, \"time_spent\": \"0:08:12\", \"epochs_done\": 41, \"batches_seen\": 1189, \"train_examples_seen\": 37515, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:10:58.136 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9766 to 0.9813\n",
            "2022-05-23 17:10:58.137 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:10:58.150 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:08:16\", \"epochs_done\": 41, \"batches_seen\": 1200, \"train_examples_seen\": 37867, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:11:08.397 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9766, \"accuracy\": 0.9265}, \"time_spent\": \"0:08:27\", \"epochs_done\": 42, \"batches_seen\": 1218, \"train_examples_seen\": 38430, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:11:17.206 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.972, \"accuracy\": 0.9118}, \"time_spent\": \"0:08:35\", \"epochs_done\": 43, \"batches_seen\": 1247, \"train_examples_seen\": 39345, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:11:26.32 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:08:44\", \"epochs_done\": 44, \"batches_seen\": 1276, \"train_examples_seen\": 40260, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:11:34.635 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9766, \"accuracy\": 0.9265}, \"time_spent\": \"0:08:53\", \"epochs_done\": 45, \"batches_seen\": 1305, \"train_examples_seen\": 41175, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:11:43.194 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9766, \"accuracy\": 0.9265}, \"time_spent\": \"0:09:01\", \"epochs_done\": 46, \"batches_seen\": 1334, \"train_examples_seen\": 42090, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:11:51.745 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:09:10\", \"epochs_done\": 47, \"batches_seen\": 1363, \"train_examples_seen\": 43005, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:00.288 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9813 to 0.986\n",
            "2022-05-23 17:12:00.289 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:12:00.297 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:09:18\", \"epochs_done\": 48, \"batches_seen\": 1392, \"train_examples_seen\": 43920, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:13.774 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:09:32\", \"epochs_done\": 49, \"batches_seen\": 1421, \"train_examples_seen\": 44835, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:22.658 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:09:41\", \"epochs_done\": 50, \"batches_seen\": 1450, \"train_examples_seen\": 45750, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:31.377 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:09:50\", \"epochs_done\": 51, \"batches_seen\": 1479, \"train_examples_seen\": 46665, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:38.496 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:09:57\", \"epochs_done\": 51, \"batches_seen\": 1500, \"train_examples_seen\": 47324, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:40.663 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:09:59\", \"epochs_done\": 52, \"batches_seen\": 1508, \"train_examples_seen\": 47580, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:49.287 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:10:07\", \"epochs_done\": 53, \"batches_seen\": 1537, \"train_examples_seen\": 48495, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:12:57.943 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:10:16\", \"epochs_done\": 54, \"batches_seen\": 1566, \"train_examples_seen\": 49410, \"impatience\": 7, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:13:06.596 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:10:25\", \"epochs_done\": 55, \"batches_seen\": 1595, \"train_examples_seen\": 50325, \"impatience\": 8, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:13:15.190 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:10:33\", \"epochs_done\": 56, \"batches_seen\": 1624, \"train_examples_seen\": 51240, \"impatience\": 9, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:13:23.779 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:10:42\", \"epochs_done\": 57, \"batches_seen\": 1653, \"train_examples_seen\": 52155, \"impatience\": 10, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:13:32.337 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:10:51\", \"epochs_done\": 58, \"batches_seen\": 1682, \"train_examples_seen\": 53070, \"impatience\": 11, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:13:40.936 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:10:59\", \"epochs_done\": 59, \"batches_seen\": 1711, \"train_examples_seen\": 53985, \"impatience\": 12, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:13:49.507 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:11:08\", \"epochs_done\": 60, \"batches_seen\": 1740, \"train_examples_seen\": 54900, \"impatience\": 13, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:13:51.37 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/tag.dict]\n",
            "2022-05-23 17:14:05.587 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:14:07.644 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:14:09.541 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/tag.dict]\n",
            "2022-05-23 17:14:27.241 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_top1000/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:14:29.125 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### other"
      ],
      "metadata": {
        "id": "jjcuynC5jpoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dir for my models\n",
        "base_config['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/fs_models/model_other'\n",
        "base_config['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/1.txt', '{MODELS_PATH}/freq_groups/0.txt']\n",
        "base_config['train']['validation_patience'] = 30\n",
        "base_config['train']['epochs'] = 60\n",
        "\n",
        "base_config['chainer']['pipe'][1]['last'] = True"
      ],
      "metadata": {
        "id": "WqkYhAsajvnd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(base_config, download=False) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItzRPmPxjwkT",
        "outputId": "a574a7c6-b2f4-4a50-d5ec-9f5673a6c491"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:14:35.155 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
            "2022-05-23 17:14:35.539 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/downloads/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:14:55.618 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 17:14:56.835 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.0, \"accuracy\": 0.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:15:08.436 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.0 to 0.4588\n",
            "2022-05-23 17:15:08.438 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:15:08.447 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.4588, \"accuracy\": 0.0882}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:15:20.935 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.4588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.4588, \"accuracy\": 0.0882}, \"time_spent\": \"0:00:26\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:15:29.230 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.4588 to 0.4974\n",
            "2022-05-23 17:15:29.231 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:15:29.239 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.4974, \"accuracy\": 0.0882}, \"time_spent\": \"0:00:34\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:15:41.377 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.4974 to 0.5622\n",
            "2022-05-23 17:15:41.378 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:15:41.386 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.5622, \"accuracy\": 0.1324}, \"time_spent\": \"0:00:46\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:15:55.121 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.5622 to 0.6515\n",
            "2022-05-23 17:15:55.125 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:15:55.131 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.6515, \"accuracy\": 0.2059}, \"time_spent\": \"0:01:00\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:16:08.390 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.6515 to 0.7198\n",
            "2022-05-23 17:16:08.392 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:16:08.399 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7198, \"accuracy\": 0.2794}, \"time_spent\": \"0:01:13\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:16:21.938 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7198 to 0.7426\n",
            "2022-05-23 17:16:21.941 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:16:21.950 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7426, \"accuracy\": 0.3235}, \"time_spent\": \"0:01:27\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:16:35.574 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7426 to 0.7618\n",
            "2022-05-23 17:16:35.579 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:16:35.587 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7618, \"accuracy\": 0.3382}, \"time_spent\": \"0:01:40\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:16:48.624 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7618 to 0.7636\n",
            "2022-05-23 17:16:48.626 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:16:48.631 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7636, \"accuracy\": 0.3676}, \"time_spent\": \"0:01:54\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:17:01.500 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7636 to 0.7828\n",
            "2022-05-23 17:17:01.502 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:17:01.512 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7828, \"accuracy\": 0.4265}, \"time_spent\": \"0:02:06\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:17:09.508 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7828 to 0.7933\n",
            "2022-05-23 17:17:09.509 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:17:09.516 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7933, \"accuracy\": 0.4412}, \"time_spent\": \"0:02:14\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9470, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:17:20.309 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7933 to 0.7968\n",
            "2022-05-23 17:17:20.312 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:17:20.325 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7968, \"accuracy\": 0.4265}, \"time_spent\": \"0:02:25\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:17:34.629 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7968 to 0.8126\n",
            "2022-05-23 17:17:34.630 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:17:34.638 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8126, \"accuracy\": 0.4559}, \"time_spent\": \"0:02:40\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:17:47.675 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8126 to 0.8284\n",
            "2022-05-23 17:17:47.679 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:17:47.685 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8284, \"accuracy\": 0.4706}, \"time_spent\": \"0:02:53\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:18:00.696 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8284 to 0.8424\n",
            "2022-05-23 17:18:00.701 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:18:00.706 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8424, \"accuracy\": 0.5147}, \"time_spent\": \"0:03:06\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:18:14.454 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8424 to 0.8511\n",
            "2022-05-23 17:18:14.455 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:18:14.478 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8511, \"accuracy\": 0.5294}, \"time_spent\": \"0:03:19\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:18:28.793 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8511 to 0.8564\n",
            "2022-05-23 17:18:28.794 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:18:28.801 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8564, \"accuracy\": 0.5294}, \"time_spent\": \"0:03:34\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:18:41.999 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8564 to 0.8581\n",
            "2022-05-23 17:18:42.0 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:18:42.14 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8581, \"accuracy\": 0.5441}, \"time_spent\": \"0:03:47\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:18:56.253 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8581 to 0.8599\n",
            "2022-05-23 17:18:56.254 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:18:56.263 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8599, \"accuracy\": 0.5441}, \"time_spent\": \"0:04:01\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:19:10.194 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8599 to 0.8932\n",
            "2022-05-23 17:19:10.198 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:19:10.203 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8932, \"accuracy\": 0.6176}, \"time_spent\": \"0:04:15\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:19:24.263 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8932 to 0.9054\n",
            "2022-05-23 17:19:24.264 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:19:24.278 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9054, \"accuracy\": 0.6324}, \"time_spent\": \"0:04:29\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:19:36.882 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9037, \"accuracy\": 0.6471}, \"time_spent\": \"0:04:42\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18927, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:19:38.955 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9054, \"accuracy\": 0.6471}, \"time_spent\": \"0:04:44\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:19:47.853 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9054, \"accuracy\": 0.6029}, \"time_spent\": \"0:04:53\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:19:56.151 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9054 to 0.9107\n",
            "2022-05-23 17:19:56.153 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:19:56.161 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9107, \"accuracy\": 0.6471}, \"time_spent\": \"0:05:01\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:20:09.855 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9107 to 0.9212\n",
            "2022-05-23 17:20:09.857 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:20:09.873 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9212, \"accuracy\": 0.6765}, \"time_spent\": \"0:05:15\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:20:24.390 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9212 to 0.9299\n",
            "2022-05-23 17:20:24.395 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:20:24.406 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9299, \"accuracy\": 0.7206}, \"time_spent\": \"0:05:29\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:20:39.212 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9212, \"accuracy\": 0.7059}, \"time_spent\": \"0:05:44\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:20:48.123 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9264, \"accuracy\": 0.6912}, \"time_spent\": \"0:05:53\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:20:56.578 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9299 to 0.9352\n",
            "2022-05-23 17:20:56.579 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:20:56.588 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9352, \"accuracy\": 0.7353}, \"time_spent\": \"0:06:01\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:21:09.841 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9352 to 0.9422\n",
            "2022-05-23 17:21:09.842 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:21:09.852 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9422, \"accuracy\": 0.7353}, \"time_spent\": \"0:06:15\", \"epochs_done\": 29, \"batches_seen\": 841, \"train_examples_seen\": 26535, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:21:24.297 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9422, \"accuracy\": 0.75}, \"time_spent\": \"0:06:29\", \"epochs_done\": 30, \"batches_seen\": 870, \"train_examples_seen\": 27450, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:21:33.652 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9422 to 0.9492\n",
            "2022-05-23 17:21:33.653 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:21:33.661 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9492, \"accuracy\": 0.75}, \"time_spent\": \"0:06:39\", \"epochs_done\": 31, \"batches_seen\": 899, \"train_examples_seen\": 28365, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:21:40.487 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9492, \"accuracy\": 0.75}, \"time_spent\": \"0:06:45\", \"epochs_done\": 31, \"batches_seen\": 900, \"train_examples_seen\": 28397, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:21:47.597 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9492 to 0.951\n",
            "2022-05-23 17:21:47.598 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:21:47.606 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.951, \"accuracy\": 0.7647}, \"time_spent\": \"0:06:52\", \"epochs_done\": 32, \"batches_seen\": 928, \"train_examples_seen\": 29280, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:22:01.190 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.951 to 0.9562\n",
            "2022-05-23 17:22:01.194 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:22:01.200 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9562, \"accuracy\": 0.7647}, \"time_spent\": \"0:07:06\", \"epochs_done\": 33, \"batches_seen\": 957, \"train_examples_seen\": 30195, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:22:15.167 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9562 to 0.958\n",
            "2022-05-23 17:22:15.169 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:22:15.181 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.958, \"accuracy\": 0.7647}, \"time_spent\": \"0:07:20\", \"epochs_done\": 34, \"batches_seen\": 986, \"train_examples_seen\": 31110, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:22:29.972 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.958 to 0.9615\n",
            "2022-05-23 17:22:29.976 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:22:29.982 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9615, \"accuracy\": 0.7794}, \"time_spent\": \"0:07:35\", \"epochs_done\": 35, \"batches_seen\": 1015, \"train_examples_seen\": 32025, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:22:44.720 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9597, \"accuracy\": 0.7647}, \"time_spent\": \"0:07:50\", \"epochs_done\": 36, \"batches_seen\": 1044, \"train_examples_seen\": 32940, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:22:53.574 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9615, \"accuracy\": 0.7794}, \"time_spent\": \"0:07:58\", \"epochs_done\": 37, \"batches_seen\": 1073, \"train_examples_seen\": 33855, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:23:02.198 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9615 to 0.9667\n",
            "2022-05-23 17:23:02.199 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:23:02.208 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9667, \"accuracy\": 0.7941}, \"time_spent\": \"0:08:07\", \"epochs_done\": 38, \"batches_seen\": 1102, \"train_examples_seen\": 34770, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:23:16.523 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9667 to 0.9702\n",
            "2022-05-23 17:23:16.524 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:23:16.535 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9702, \"accuracy\": 0.8382}, \"time_spent\": \"0:08:21\", \"epochs_done\": 39, \"batches_seen\": 1131, \"train_examples_seen\": 35685, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:23:30.947 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9685, \"accuracy\": 0.8088}, \"time_spent\": \"0:08:36\", \"epochs_done\": 40, \"batches_seen\": 1160, \"train_examples_seen\": 36600, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:23:39.758 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9667, \"accuracy\": 0.8088}, \"time_spent\": \"0:08:45\", \"epochs_done\": 41, \"batches_seen\": 1189, \"train_examples_seen\": 37515, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:23:44.872 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9702 to 0.972\n",
            "2022-05-23 17:23:44.875 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:23:44.886 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.972, \"accuracy\": 0.8529}, \"time_spent\": \"0:08:50\", \"epochs_done\": 41, \"batches_seen\": 1200, \"train_examples_seen\": 37854, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:23:54.395 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.972 to 0.9737\n",
            "2022-05-23 17:23:54.396 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:23:54.406 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9737, \"accuracy\": 0.8382}, \"time_spent\": \"0:08:59\", \"epochs_done\": 42, \"batches_seen\": 1218, \"train_examples_seen\": 38430, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:24:10.348 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9737 to 0.9772\n",
            "2022-05-23 17:24:10.352 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:24:10.361 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9772, \"accuracy\": 0.8676}, \"time_spent\": \"0:09:15\", \"epochs_done\": 43, \"batches_seen\": 1247, \"train_examples_seen\": 39345, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:24:25.377 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.972, \"accuracy\": 0.8529}, \"time_spent\": \"0:09:30\", \"epochs_done\": 44, \"batches_seen\": 1276, \"train_examples_seen\": 40260, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:24:34.497 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9772 to 0.979\n",
            "2022-05-23 17:24:34.499 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:24:34.507 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.979, \"accuracy\": 0.8824}, \"time_spent\": \"0:09:39\", \"epochs_done\": 45, \"batches_seen\": 1305, \"train_examples_seen\": 41175, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:24:48.96 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.979, \"accuracy\": 0.8824}, \"time_spent\": \"0:09:53\", \"epochs_done\": 46, \"batches_seen\": 1334, \"train_examples_seen\": 42090, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:24:57.16 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.979 to 0.9807\n",
            "2022-05-23 17:24:57.17 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:24:57.25 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9807, \"accuracy\": 0.8971}, \"time_spent\": \"0:10:02\", \"epochs_done\": 47, \"batches_seen\": 1363, \"train_examples_seen\": 43005, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:25:11.347 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9807 to 0.9825\n",
            "2022-05-23 17:25:11.349 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:25:11.361 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9825, \"accuracy\": 0.9118}, \"time_spent\": \"0:10:16\", \"epochs_done\": 48, \"batches_seen\": 1392, \"train_examples_seen\": 43920, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:25:26.797 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9825 to 0.986\n",
            "2022-05-23 17:25:26.799 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:25:26.811 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.8971}, \"time_spent\": \"0:10:32\", \"epochs_done\": 49, \"batches_seen\": 1421, \"train_examples_seen\": 44835, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:25:41.714 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9842, \"accuracy\": 0.9118}, \"time_spent\": \"0:10:47\", \"epochs_done\": 50, \"batches_seen\": 1450, \"train_examples_seen\": 45750, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:25:50.918 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9118}, \"time_spent\": \"0:10:56\", \"epochs_done\": 51, \"batches_seen\": 1479, \"train_examples_seen\": 46665, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:25:56.805 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9825, \"accuracy\": 0.8971}, \"time_spent\": \"0:11:02\", \"epochs_done\": 51, \"batches_seen\": 1500, \"train_examples_seen\": 47337, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:25:59.756 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9842, \"accuracy\": 0.9118}, \"time_spent\": \"0:11:05\", \"epochs_done\": 52, \"batches_seen\": 1508, \"train_examples_seen\": 47580, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:26:08.399 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9118}, \"time_spent\": \"0:11:13\", \"epochs_done\": 53, \"batches_seen\": 1537, \"train_examples_seen\": 48495, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:26:17.62 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.986 to 0.9877\n",
            "2022-05-23 17:26:17.64 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:26:17.71 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:11:22\", \"epochs_done\": 54, \"batches_seen\": 1566, \"train_examples_seen\": 49410, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:26:31.362 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9825, \"accuracy\": 0.8971}, \"time_spent\": \"0:11:36\", \"epochs_done\": 55, \"batches_seen\": 1595, \"train_examples_seen\": 50325, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:26:40.631 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:11:46\", \"epochs_done\": 56, \"batches_seen\": 1624, \"train_examples_seen\": 51240, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:26:49.278 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9118}, \"time_spent\": \"0:11:54\", \"epochs_done\": 57, \"batches_seen\": 1653, \"train_examples_seen\": 52155, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:26:58.4 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:12:03\", \"epochs_done\": 58, \"batches_seen\": 1682, \"train_examples_seen\": 53070, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:27:06.724 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:12:12\", \"epochs_done\": 59, \"batches_seen\": 1711, \"train_examples_seen\": 53985, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:27:15.471 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:12:20\", \"epochs_done\": 60, \"batches_seen\": 1740, \"train_examples_seen\": 54900, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:27:17.307 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n",
            "2022-05-23 17:27:31.775 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:27:33.813 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:27:35.391 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n",
            "2022-05-23 17:27:53.416 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:27:55.745 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_config['train']['validation_patience'] = 10\n",
        "base_config['train']['epochs'] = 10\n",
        "\n",
        "model = train_model(base_config, download=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDTgHAgU9ag1",
        "outputId": "e7bda879-0bd4-4e67-ffd3-1443e1f730da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:28:10.98 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
            "2022-05-23 17:28:10.459 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n",
            "2022-05-23 17:28:10.483 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n",
            "2022-05-23 17:28:27.579 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:28:29.362 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 17:28:30.515 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:28:42.85 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 1, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:28:50.84 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9842, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:21\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 2, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:28:58.61 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:29\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 3, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:29:06.54 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:37\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 4, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:29:14.71 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:45\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 5, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:29:22.105 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:53\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 6, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:29:30.163 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9877 to 0.9895\n",
            "2022-05-23 17:29:30.168 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:29:30.173 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:01\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 0, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:29:43.324 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:14\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 1, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:29:51.765 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:23\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 2, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:29:59.960 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:31\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 3, \"patience_limit\": 10}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:30:00.945 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n",
            "2022-05-23 17:30:16.425 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:30:18.180 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:30:19.755 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/tag.dict]\n",
            "2022-05-23 17:30:37.673 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/fs_models/model_other/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:30:39.513 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline General"
      ],
      "metadata": {
        "id": "bhyIG3at48Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Scratch General Model to compare with"
      ],
      "metadata": {
        "id": "y2XkriFeXPqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empty_file = open('empty_file.txt', 'w') #take tokens apart from the ones in this file => all\n"
      ],
      "metadata": {
        "id": "0jV-yjM347qY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### dir for my models\n",
        "base_config['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/baseline'\n",
        "base_config['chainer']['pipe'][1]['topk_tokens_path'] = ['{ROOT_PATH}/empty_file.txt']\n",
        "base_config['train']['validation_patience'] = 30\n",
        "base_config['train']['epochs'] = 60\n",
        "base_config['chainer']['pipe'][1]['last'] = True #take all tokens"
      ],
      "metadata": {
        "id": "NwCGpbKVZ8Pk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = train_model(base_config, download=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD-XSLchbULm",
        "outputId": "1f9d3a74-08aa-4b3b-97cf-2ebd9a001f4e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:31:06.203 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
            "2022-05-23 17:31:06.620 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/downloads/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:31:25.731 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 17:31:26.957 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.0037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.0037, \"accuracy\": 0.0}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:31:38.518 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.0037 to 0.3232\n",
            "2022-05-23 17:31:38.519 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:31:38.525 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.3232, \"accuracy\": 0.0}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:31:51.244 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.3232 to 0.3343\n",
            "2022-05-23 17:31:51.247 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:31:51.262 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.3343, \"accuracy\": 0.0}, \"time_spent\": \"0:00:26\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:32:04.843 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.3343 to 0.4349\n",
            "2022-05-23 17:32:04.844 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:32:04.854 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.4349, \"accuracy\": 0.0}, \"time_spent\": \"0:00:40\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:32:18.544 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.4349 to 0.5743\n",
            "2022-05-23 17:32:18.546 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:32:18.555 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.5743, \"accuracy\": 0.0}, \"time_spent\": \"0:00:53\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:32:31.320 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.5743 to 0.651\n",
            "2022-05-23 17:32:31.322 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:32:31.333 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.651, \"accuracy\": 0.0588}, \"time_spent\": \"0:01:06\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:32:45.128 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.651 to 0.7054\n",
            "2022-05-23 17:32:45.129 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:32:45.136 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7054, \"accuracy\": 0.0882}, \"time_spent\": \"0:01:20\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:32:59.137 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7054 to 0.7147\n",
            "2022-05-23 17:32:59.141 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:32:59.151 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7147, \"accuracy\": 0.0882}, \"time_spent\": \"0:01:34\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:33:12.175 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7147 to 0.7479\n",
            "2022-05-23 17:33:12.177 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:33:12.184 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7479, \"accuracy\": 0.1176}, \"time_spent\": \"0:01:47\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:33:25.400 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7479 to 0.7747\n",
            "2022-05-23 17:33:25.401 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:33:25.408 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7747, \"accuracy\": 0.1765}, \"time_spent\": \"0:02:00\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:33:38.208 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7747 to 0.7959\n",
            "2022-05-23 17:33:38.214 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:33:38.225 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.7959, \"accuracy\": 0.1912}, \"time_spent\": \"0:02:13\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:33:46.642 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.7959 to 0.807\n",
            "2022-05-23 17:33:46.645 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:33:46.651 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.807, \"accuracy\": 0.2206}, \"time_spent\": \"0:02:21\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9470, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:33:57.143 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.807 to 0.8153\n",
            "2022-05-23 17:33:57.144 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:33:57.151 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8153, \"accuracy\": 0.2059}, \"time_spent\": \"0:02:32\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:34:11.472 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8153 to 0.8264\n",
            "2022-05-23 17:34:11.474 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:34:11.483 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8264, \"accuracy\": 0.2941}, \"time_spent\": \"0:02:46\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:34:25.255 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8264 to 0.8403\n",
            "2022-05-23 17:34:25.258 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:34:25.274 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8403, \"accuracy\": 0.2941}, \"time_spent\": \"0:03:00\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:34:40.272 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8403 to 0.8467\n",
            "2022-05-23 17:34:40.277 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:34:40.286 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8467, \"accuracy\": 0.3235}, \"time_spent\": \"0:03:15\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:34:54.388 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8467 to 0.8633\n",
            "2022-05-23 17:34:54.389 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:34:54.402 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8633, \"accuracy\": 0.3235}, \"time_spent\": \"0:03:29\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:35:08.38 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8633 to 0.8818\n",
            "2022-05-23 17:35:08.39 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:35:08.47 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8818, \"accuracy\": 0.3529}, \"time_spent\": \"0:03:43\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:35:22.159 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8818 to 0.8864\n",
            "2022-05-23 17:35:22.163 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:35:22.172 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8864, \"accuracy\": 0.4118}, \"time_spent\": \"0:03:57\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:35:37.216 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8864 to 0.8984\n",
            "2022-05-23 17:35:37.217 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:35:37.226 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8984, \"accuracy\": 0.4265}, \"time_spent\": \"0:04:12\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:35:51.157 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.8984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.8957, \"accuracy\": 0.4118}, \"time_spent\": \"0:04:26\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:35:59.694 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.8984 to 0.9003\n",
            "2022-05-23 17:35:59.695 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:35:59.703 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9003, \"accuracy\": 0.4412}, \"time_spent\": \"0:04:34\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:36:10.663 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9003 to 0.9095\n",
            "2022-05-23 17:36:10.667 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:36:10.672 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9095, \"accuracy\": 0.4412}, \"time_spent\": \"0:04:45\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18927, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:36:19.65 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.903, \"accuracy\": 0.4265}, \"time_spent\": \"0:04:54\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:36:27.903 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.903, \"accuracy\": 0.4412}, \"time_spent\": \"0:05:03\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:36:36.496 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9095 to 0.9123\n",
            "2022-05-23 17:36:36.497 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:36:36.507 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9123, \"accuracy\": 0.4412}, \"time_spent\": \"0:05:11\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:36:49.572 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9123 to 0.9187\n",
            "2022-05-23 17:36:49.574 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:36:49.585 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9187, \"accuracy\": 0.4853}, \"time_spent\": \"0:05:24\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:37:03.801 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9187 to 0.9243\n",
            "2022-05-23 17:37:03.804 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:37:03.815 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9243, \"accuracy\": 0.4706}, \"time_spent\": \"0:05:39\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:37:17.954 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9243 to 0.9271\n",
            "2022-05-23 17:37:17.956 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:37:17.964 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9271, \"accuracy\": 0.4853}, \"time_spent\": \"0:05:53\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:37:32.597 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9271 to 0.9289\n",
            "2022-05-23 17:37:32.600 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:37:32.610 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9289, \"accuracy\": 0.5147}, \"time_spent\": \"0:06:07\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:37:47.744 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9289 to 0.9372\n",
            "2022-05-23 17:37:47.745 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:37:47.754 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9372, \"accuracy\": 0.5441}, \"time_spent\": \"0:06:23\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:38:02.582 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9372 to 0.9409\n",
            "2022-05-23 17:38:02.584 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:38:02.589 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9409, \"accuracy\": 0.5441}, \"time_spent\": \"0:06:37\", \"epochs_done\": 29, \"batches_seen\": 841, \"train_examples_seen\": 26535, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:38:17.49 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9409 to 0.9418\n",
            "2022-05-23 17:38:17.51 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:38:17.63 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9418, \"accuracy\": 0.5441}, \"time_spent\": \"0:06:52\", \"epochs_done\": 30, \"batches_seen\": 870, \"train_examples_seen\": 27450, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:38:31.765 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9418 to 0.9446\n",
            "2022-05-23 17:38:31.766 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:38:31.778 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9446, \"accuracy\": 0.5294}, \"time_spent\": \"0:07:07\", \"epochs_done\": 31, \"batches_seen\": 899, \"train_examples_seen\": 28365, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:38:40.128 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9446 to 0.9474\n",
            "2022-05-23 17:38:40.131 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:38:40.138 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9474, \"accuracy\": 0.5588}, \"time_spent\": \"0:07:15\", \"epochs_done\": 31, \"batches_seen\": 900, \"train_examples_seen\": 28397, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:38:52.903 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9474 to 0.9511\n",
            "2022-05-23 17:38:52.904 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:38:52.918 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9511, \"accuracy\": 0.5735}, \"time_spent\": \"0:07:28\", \"epochs_done\": 32, \"batches_seen\": 928, \"train_examples_seen\": 29280, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:39:07.110 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9511 to 0.9557\n",
            "2022-05-23 17:39:07.112 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:39:07.122 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9557, \"accuracy\": 0.6029}, \"time_spent\": \"0:07:42\", \"epochs_done\": 33, \"batches_seen\": 957, \"train_examples_seen\": 30195, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:39:21.188 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9557 to 0.9566\n",
            "2022-05-23 17:39:21.189 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:39:21.201 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9566, \"accuracy\": 0.6029}, \"time_spent\": \"0:07:56\", \"epochs_done\": 34, \"batches_seen\": 986, \"train_examples_seen\": 31110, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:39:36.805 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9566 to 0.9594\n",
            "2022-05-23 17:39:36.809 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:39:36.814 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9594, \"accuracy\": 0.6176}, \"time_spent\": \"0:08:12\", \"epochs_done\": 35, \"batches_seen\": 1015, \"train_examples_seen\": 32025, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:39:51.0 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9594 to 0.9631\n",
            "2022-05-23 17:39:51.2 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:39:51.13 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9631, \"accuracy\": 0.6618}, \"time_spent\": \"0:08:26\", \"epochs_done\": 36, \"batches_seen\": 1044, \"train_examples_seen\": 32940, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:40:06.289 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9631 to 0.9658\n",
            "2022-05-23 17:40:06.291 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:40:06.300 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9658, \"accuracy\": 0.6765}, \"time_spent\": \"0:08:41\", \"epochs_done\": 37, \"batches_seen\": 1073, \"train_examples_seen\": 33855, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:40:21.166 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9649, \"accuracy\": 0.6618}, \"time_spent\": \"0:08:56\", \"epochs_done\": 38, \"batches_seen\": 1102, \"train_examples_seen\": 34770, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:40:30.322 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9658, \"accuracy\": 0.6765}, \"time_spent\": \"0:09:05\", \"epochs_done\": 39, \"batches_seen\": 1131, \"train_examples_seen\": 35685, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:40:38.699 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9658 to 0.9705\n",
            "2022-05-23 17:40:38.701 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:40:38.708 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9705, \"accuracy\": 0.7059}, \"time_spent\": \"0:09:13\", \"epochs_done\": 40, \"batches_seen\": 1160, \"train_examples_seen\": 36600, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:40:52.765 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9705 to 0.9714\n",
            "2022-05-23 17:40:52.767 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:40:52.785 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9714, \"accuracy\": 0.7206}, \"time_spent\": \"0:09:28\", \"epochs_done\": 41, \"batches_seen\": 1189, \"train_examples_seen\": 37515, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:41:03.514 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9705, \"accuracy\": 0.7206}, \"time_spent\": \"0:09:38\", \"epochs_done\": 41, \"batches_seen\": 1200, \"train_examples_seen\": 37867, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:41:08.552 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9677, \"accuracy\": 0.6765}, \"time_spent\": \"0:09:43\", \"epochs_done\": 42, \"batches_seen\": 1218, \"train_examples_seen\": 38430, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:41:18.110 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9714 to 0.9723\n",
            "2022-05-23 17:41:18.116 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:41:18.121 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9723, \"accuracy\": 0.7059}, \"time_spent\": \"0:09:53\", \"epochs_done\": 43, \"batches_seen\": 1247, \"train_examples_seen\": 39345, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:41:31.766 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9723 to 0.9732\n",
            "2022-05-23 17:41:31.770 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:41:31.776 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9732, \"accuracy\": 0.75}, \"time_spent\": \"0:10:07\", \"epochs_done\": 44, \"batches_seen\": 1276, \"train_examples_seen\": 40260, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:41:47.720 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9732 to 0.9741\n",
            "2022-05-23 17:41:47.721 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:41:47.734 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9741, \"accuracy\": 0.7647}, \"time_spent\": \"0:10:22\", \"epochs_done\": 45, \"batches_seen\": 1305, \"train_examples_seen\": 41175, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:42:01.995 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9741, \"accuracy\": 0.75}, \"time_spent\": \"0:10:37\", \"epochs_done\": 46, \"batches_seen\": 1334, \"train_examples_seen\": 42090, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:42:10.986 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9741 to 0.9788\n",
            "2022-05-23 17:42:10.987 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:42:10.995 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9788, \"accuracy\": 0.7941}, \"time_spent\": \"0:10:46\", \"epochs_done\": 47, \"batches_seen\": 1363, \"train_examples_seen\": 43005, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:42:25.530 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9741, \"accuracy\": 0.7353}, \"time_spent\": \"0:11:00\", \"epochs_done\": 48, \"batches_seen\": 1392, \"train_examples_seen\": 43920, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:42:34.583 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9778, \"accuracy\": 0.75}, \"time_spent\": \"0:11:09\", \"epochs_done\": 49, \"batches_seen\": 1421, \"train_examples_seen\": 44835, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:42:43.102 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9788 to 0.9806\n",
            "2022-05-23 17:42:43.103 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:42:43.112 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9806, \"accuracy\": 0.7941}, \"time_spent\": \"0:11:18\", \"epochs_done\": 50, \"batches_seen\": 1450, \"train_examples_seen\": 45750, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:42:57.915 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9806 to 0.9834\n",
            "2022-05-23 17:42:57.917 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:42:57.930 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9834, \"accuracy\": 0.8088}, \"time_spent\": \"0:11:33\", \"epochs_done\": 51, \"batches_seen\": 1479, \"train_examples_seen\": 46665, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:43:11.206 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9806, \"accuracy\": 0.7941}, \"time_spent\": \"0:11:46\", \"epochs_done\": 51, \"batches_seen\": 1500, \"train_examples_seen\": 47324, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:43:12.777 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9834, \"accuracy\": 0.8088}, \"time_spent\": \"0:11:48\", \"epochs_done\": 52, \"batches_seen\": 1508, \"train_examples_seen\": 47580, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:43:22.334 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9834 to 0.9843\n",
            "2022-05-23 17:43:22.335 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:43:22.344 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9843, \"accuracy\": 0.7941}, \"time_spent\": \"0:11:57\", \"epochs_done\": 53, \"batches_seen\": 1537, \"train_examples_seen\": 48495, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:43:36.343 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9825, \"accuracy\": 0.7941}, \"time_spent\": \"0:12:11\", \"epochs_done\": 54, \"batches_seen\": 1566, \"train_examples_seen\": 49410, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:43:45.472 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9834, \"accuracy\": 0.7941}, \"time_spent\": \"0:12:20\", \"epochs_done\": 55, \"batches_seen\": 1595, \"train_examples_seen\": 50325, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:43:54.42 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9815, \"accuracy\": 0.7794}, \"time_spent\": \"0:12:29\", \"epochs_done\": 56, \"batches_seen\": 1624, \"train_examples_seen\": 51240, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:44:02.607 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9843, \"accuracy\": 0.7941}, \"time_spent\": \"0:12:37\", \"epochs_done\": 57, \"batches_seen\": 1653, \"train_examples_seen\": 52155, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:44:11.264 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9806, \"accuracy\": 0.7794}, \"time_spent\": \"0:12:46\", \"epochs_done\": 58, \"batches_seen\": 1682, \"train_examples_seen\": 53070, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:44:19.905 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9825, \"accuracy\": 0.7941}, \"time_spent\": \"0:12:55\", \"epochs_done\": 59, \"batches_seen\": 1711, \"train_examples_seen\": 53985, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:44:28.607 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9843 to 0.9861\n",
            "2022-05-23 17:44:28.611 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:44:28.615 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9861, \"accuracy\": 0.7941}, \"time_spent\": \"0:13:03\", \"epochs_done\": 60, \"batches_seen\": 1740, \"train_examples_seen\": 54900, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:44:35.919 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/tag.dict]\n",
            "2022-05-23 17:44:53.617 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:44:55.642 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9861, \"accuracy\": 0.7941}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:44:57.225 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/tag.dict]\n",
            "2022-05-23 17:45:15.769 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:45:17.599 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_config['train']['validation_patience'] = 30\n",
        "base_config['train']['epochs'] = 30"
      ],
      "metadata": {
        "id": "rbGywwl_EbwD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = train_model(base_config, download=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeJZBVHAEiz_",
        "outputId": "cbd5767c-7e50-4272-9115-9808b58c9ad8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:46:48.335 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:150: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:46:48.722 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/tag.dict]\n",
            "2022-05-23 17:46:48.745 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From <ipython-input-4-5405f03ac0b9>:54: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-4-5405f03ac0b9>:228: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:47:09.781 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:47:11.650 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 17:47:13.555 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.9861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9861, \"accuracy\": 0.7941}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:178: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:47:24.419 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9852, \"accuracy\": 0.7941}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 1, \"patience_limit\": 30}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:211: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:47:32.36 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9861, \"accuracy\": 0.8088}, \"time_spent\": \"0:00:21\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:47:39.668 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9843, \"accuracy\": 0.7794}, \"time_spent\": \"0:00:29\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:47:47.424 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9861 to 0.988\n",
            "2022-05-23 17:47:47.425 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:47:47.433 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.988, \"accuracy\": 0.8088}, \"time_spent\": \"0:00:36\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:47:59.472 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9861, \"accuracy\": 0.7941}, \"time_spent\": \"0:00:48\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:07.305 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9871, \"accuracy\": 0.8088}, \"time_spent\": \"0:00:56\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:15.273 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.988, \"accuracy\": 0.8088}, \"time_spent\": \"0:01:04\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:23.41 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.988 to 0.9889\n",
            "2022-05-23 17:48:23.44 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:48:23.49 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:01:12\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:35.575 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:01:24\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:43.620 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:01:32\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:46.918 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:01:36\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9470, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:51.597 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.988, \"accuracy\": 0.8088}, \"time_spent\": \"0:01:40\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:48:59.449 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best per_token_accuracy from 0.9889 to 0.9898\n",
            "2022-05-23 17:48:59.450 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2022-05-23 17:48:59.458 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:01:48\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:49:11.347 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:02:00\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:49:19.595 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:02:08\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:49:27.558 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8382}, \"time_spent\": \"0:02:16\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:49:35.529 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:02:24\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:49:43.455 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:02:32\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:49:51.396 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:02:40\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:49:59.318 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:02:48\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 7, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:07.254 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:02:56\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 8, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:13.653 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:02\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18927, \"impatience\": 9, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:15.386 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:04\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 10, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:23.370 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:12\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 11, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:31.335 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:20\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 12, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:39.369 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:28\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 13, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:47.468 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:36\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 14, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:50:55.545 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:44\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 15, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:51:03.670 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:03:53\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 16, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:51:11.859 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:04:01\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 17, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:51:19.983 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:04:09\", \"epochs_done\": 29, \"batches_seen\": 841, \"train_examples_seen\": 26535, \"impatience\": 18, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:51:28.165 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9889, \"accuracy\": 0.8235}, \"time_spent\": \"0:04:17\", \"epochs_done\": 30, \"batches_seen\": 870, \"train_examples_seen\": 27450, \"impatience\": 19, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:51:29.189 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/tag.dict]\n",
            "2022-05-23 17:51:43.168 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:51:44.945 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9898, \"accuracy\": 0.8382}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:51:46.442 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/tag.dict]\n",
            "2022-05-23 17:52:04.168 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:52:05.915 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FineTune Models"
      ],
      "metadata": {
        "id": "6OGAEZQpBVZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load baseline general model and fit it on groups separately"
      ],
      "metadata": {
        "id": "pXFjfVeTSdzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_config['chainer']['pipe'][3]['load_path'] = '{MODELS_PATH}/baseline/model'"
      ],
      "metadata": {
        "id": "HQVGHaTTTdJt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### top100"
      ],
      "metadata": {
        "id": "4KxssxMlvDsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dir for my models\n",
        "\n",
        "base_config['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/finetuned_models/model_top100'\n",
        "base_config['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt']\n",
        "base_config['train']['validation_patience'] = 30\n",
        "base_config['train']['epochs'] = 60\n",
        "base_config['chainer']['pipe'][1]['last'] = False"
      ],
      "metadata": {
        "id": "LrDJnvqMrgAy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(base_config, download=False)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAVNphDswarw",
        "outputId": "fa877eac-1f07-4010-900d-21d6ac230645"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:53:05.406 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
            "2022-05-23 17:53:05.798 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top100/tag.dict]\n",
            "2022-05-23 17:53:25.612 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:53:27.392 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 17:53:28.468 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:53:39.652 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:53:47.424 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:21\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:53:55.208 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:28\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:02.997 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:36\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:10.794 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:44\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:18.661 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:52\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:26.664 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:00\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 7, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:34.560 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:08\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 8, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:42.521 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:16\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 9, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:50.529 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:24\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 10, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:53.95 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:26\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9470, \"impatience\": 11, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:54:58.726 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:32\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 12, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:55:06.697 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:40\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 13, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:55:14.709 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:48\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 14, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:55:22.701 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:01:56\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 15, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:55:30.696 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:04\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 16, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:55:38.716 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:12\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 17, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:55:46.744 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:20\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 18, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:55:54.801 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:28\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 19, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:02.995 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:36\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 20, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:11.139 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:44\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 21, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:17.279 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:50\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18927, \"impatience\": 22, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:19.584 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:02:53\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 23, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:27.786 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:03:01\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 24, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:35.976 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:03:09\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 25, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:44.113 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:03:17\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 26, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:56:52.201 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:03:25\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 27, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:57:00.282 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:03:33\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 28, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:57:08.372 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:03:41\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 29, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:57:16.437 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9933\n",
            "2022-05-23 17:57:16.917 INFO in 'deeppavlov.core.models.lr_scheduled_model'['lr_scheduled_model'] at line 429: New learning rate dividor = 1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:03:50\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 30, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:57:17.395 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 329: Ran out of patience\n",
            "2022-05-23 17:57:17.914 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top100/tag.dict]\n",
            "2022-05-23 17:57:17.925 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 45: Load path '/content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model' differs from save path '/content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top100/model' in 'infer' mode for MyBertSequenceTagger.\n",
            "2022-05-23 17:57:36.298 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:57:38.87 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9933, \"accuracy\": 0.9706}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:57:39.594 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top100/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/downloads/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:58:01.658 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### top1000"
      ],
      "metadata": {
        "id": "mpMTEudm7d9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dir for my models\n",
        "\n",
        "base_config['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/finetuned_models/model_top1000'\n",
        "base_config['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/1.txt']\n",
        "base_config['train']['validation_patience'] = 30\n",
        "base_config['train']['epochs'] = 60\n",
        "base_config['chainer']['pipe'][1]['last'] = False"
      ],
      "metadata": {
        "id": "Fg97uNC57dvg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(base_config, download=False)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj99XN_z7l7C",
        "outputId": "0220bdb2-3102-4adc-e88f-23fe8f9e823e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:59:44.766 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:150: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 17:59:45.144 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top1000/tag.dict]\n",
            "2022-05-23 17:59:45.166 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top1000/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From <ipython-input-3-5405f03ac0b9>:54: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-5405f03ac0b9>:228: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:02.597 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:04.439 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 18:00:05.935 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:178: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:16.954 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 1, \"patience_limit\": 30}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:211: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:24.716 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:21\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:32.504 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:29\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:40.290 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9766, \"accuracy\": 0.9265}, \"time_spent\": \"0:00:36\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:48.348 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:00:44\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:00:56.237 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:00:52\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:04.149 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:00\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 7, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:12.85 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:08\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 8, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:20.72 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9813, \"accuracy\": 0.9412}, \"time_spent\": \"0:01:16\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 9, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:28.55 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:24\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 10, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:31.223 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:27\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9470, \"impatience\": 11, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:36.196 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:32\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 12, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:44.212 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:40\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 13, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:01:52.230 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:48\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 14, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:00.254 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:01:56\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 15, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:08.288 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:04\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 16, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:16.318 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:12\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 17, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:24.529 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:21\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 18, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:32.643 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:29\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 19, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:40.779 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:37\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 20, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:48.957 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:45\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 21, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:55.213 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:51\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18927, \"impatience\": 22, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:02:57.318 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:02:53\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 23, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:05.505 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:03:02\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 24, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:13.649 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:03:10\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 25, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:21.764 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:03:18\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 26, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:29.903 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:03:26\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 27, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:38.78 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:03:34\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 28, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:46.239 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:03:42\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 29, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:54.438 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.986\n",
            "2022-05-23 18:03:55.16 INFO in 'deeppavlov.core.models.lr_scheduled_model'['lr_scheduled_model'] at line 429: New learning rate dividor = 1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:03:50\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 30, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:03:55.571 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 329: Ran out of patience\n",
            "2022-05-23 18:03:56.87 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top1000/tag.dict]\n",
            "2022-05-23 18:03:56.98 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 45: Load path '/content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model' differs from save path '/content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top1000/model' in 'infer' mode for MyBertSequenceTagger.\n",
            "2022-05-23 18:04:10.197 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:04:11.977 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.986, \"accuracy\": 0.9559}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:04:13.509 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_top1000/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/downloads/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:04:31.597 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### top1000+"
      ],
      "metadata": {
        "id": "txfdOKICAuMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### dir for my models\n",
        "base_config['metadata']['variables']['WORK_PATH'] = '{MODELS_PATH}/finetuned_models/model_other'\n",
        "base_config['chainer']['pipe'][1]['topk_tokens_path'] = ['{MODELS_PATH}/freq_groups/0.txt', '{MODELS_PATH}/freq_groups/1.txt']\n",
        "base_config['train']['validation_patience'] = 30\n",
        "base_config['train']['epochs'] = 60\n",
        "base_config['chainer']['pipe'][1]['last'] = True"
      ],
      "metadata": {
        "id": "OTIym1LtAwdv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(base_config, download=False)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPRnK0N5A24R",
        "outputId": "90ded4f3-e2fe-42a0-c1f6-2abbe5b28e03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:04:54.112 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
            "2022-05-23 18:04:54.487 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_other/tag.dict]\n",
            "2022-05-23 18:05:13.30 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:05:14.793 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
            "2022-05-23 18:05:15.862 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:05:27.103 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:13\", \"epochs_done\": 1, \"batches_seen\": 29, \"train_examples_seen\": 915, \"impatience\": 1, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:05:35.141 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:21\", \"epochs_done\": 2, \"batches_seen\": 58, \"train_examples_seen\": 1830, \"impatience\": 2, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:05:43.61 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9877, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:29\", \"epochs_done\": 3, \"batches_seen\": 87, \"train_examples_seen\": 2745, \"impatience\": 3, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:05:51.40 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:37\", \"epochs_done\": 4, \"batches_seen\": 116, \"train_examples_seen\": 3660, \"impatience\": 4, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:05:59.93 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:45\", \"epochs_done\": 5, \"batches_seen\": 145, \"train_examples_seen\": 4575, \"impatience\": 5, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:07.115 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:53\", \"epochs_done\": 6, \"batches_seen\": 174, \"train_examples_seen\": 5490, \"impatience\": 6, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:15.174 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:01\", \"epochs_done\": 7, \"batches_seen\": 203, \"train_examples_seen\": 6405, \"impatience\": 7, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:23.244 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:09\", \"epochs_done\": 8, \"batches_seen\": 232, \"train_examples_seen\": 7320, \"impatience\": 8, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:31.428 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:17\", \"epochs_done\": 9, \"batches_seen\": 261, \"train_examples_seen\": 8235, \"impatience\": 9, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:39.486 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:25\", \"epochs_done\": 10, \"batches_seen\": 290, \"train_examples_seen\": 9150, \"impatience\": 10, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:41.996 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:28\", \"epochs_done\": 10, \"batches_seen\": 300, \"train_examples_seen\": 9470, \"impatience\": 11, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:47.749 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:33\", \"epochs_done\": 11, \"batches_seen\": 319, \"train_examples_seen\": 10065, \"impatience\": 12, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:06:55.832 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:42\", \"epochs_done\": 12, \"batches_seen\": 348, \"train_examples_seen\": 10980, \"impatience\": 13, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:07:03.922 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:50\", \"epochs_done\": 13, \"batches_seen\": 377, \"train_examples_seen\": 11895, \"impatience\": 14, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:07:12.158 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:01:58\", \"epochs_done\": 14, \"batches_seen\": 406, \"train_examples_seen\": 12810, \"impatience\": 15, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:07:20.427 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:06\", \"epochs_done\": 15, \"batches_seen\": 435, \"train_examples_seen\": 13725, \"impatience\": 16, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:07:28.653 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:14\", \"epochs_done\": 16, \"batches_seen\": 464, \"train_examples_seen\": 14640, \"impatience\": 17, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:07:36.929 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:23\", \"epochs_done\": 17, \"batches_seen\": 493, \"train_examples_seen\": 15555, \"impatience\": 18, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:07:45.168 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:31\", \"epochs_done\": 18, \"batches_seen\": 522, \"train_examples_seen\": 16470, \"impatience\": 19, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:07:53.379 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:39\", \"epochs_done\": 19, \"batches_seen\": 551, \"train_examples_seen\": 17385, \"impatience\": 20, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:01.538 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:47\", \"epochs_done\": 20, \"batches_seen\": 580, \"train_examples_seen\": 18300, \"impatience\": 21, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:06.230 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:52\", \"epochs_done\": 20, \"batches_seen\": 600, \"train_examples_seen\": 18940, \"impatience\": 22, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:09.896 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:02:56\", \"epochs_done\": 21, \"batches_seen\": 609, \"train_examples_seen\": 19215, \"impatience\": 23, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:18.68 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:03:04\", \"epochs_done\": 22, \"batches_seen\": 638, \"train_examples_seen\": 20130, \"impatience\": 24, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:26.242 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:03:12\", \"epochs_done\": 23, \"batches_seen\": 667, \"train_examples_seen\": 21045, \"impatience\": 25, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:34.433 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:03:20\", \"epochs_done\": 24, \"batches_seen\": 696, \"train_examples_seen\": 21960, \"impatience\": 26, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:42.653 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:03:28\", \"epochs_done\": 25, \"batches_seen\": 725, \"train_examples_seen\": 22875, \"impatience\": 27, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:50.988 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:03:37\", \"epochs_done\": 26, \"batches_seen\": 754, \"train_examples_seen\": 23790, \"impatience\": 28, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:08:59.266 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:03:45\", \"epochs_done\": 27, \"batches_seen\": 783, \"train_examples_seen\": 24705, \"impatience\": 29, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:09:07.583 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the per_token_accuracy of 0.9895\n",
            "2022-05-23 18:09:08.122 INFO in 'deeppavlov.core.models.lr_scheduled_model'['lr_scheduled_model'] at line 429: New learning rate dividor = 1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:03:53\", \"epochs_done\": 28, \"batches_seen\": 812, \"train_examples_seen\": 25620, \"impatience\": 30, \"patience_limit\": 30}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:09:08.657 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 329: Ran out of patience\n",
            "2022-05-23 18:09:09.141 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_other/tag.dict]\n",
            "2022-05-23 18:09:09.152 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 45: Load path '/content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model' differs from save path '/content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_other/model' in 'infer' mode for MyBertSequenceTagger.\n",
            "2022-05-23 18:09:26.91 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/baseline/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:09:27.884 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 68, \"metrics\": {\"per_token_accuracy\": 0.9895, \"accuracy\": 0.9118}, \"time_spent\": \"0:00:02\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:09:29.445 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/drive/.shortcut-targets-by-id/1GTaDlf0tB2yaTzRccvhA3cso1sKiTY9u/models_diploma/GramEval_poetry/finetuned_models/model_other/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/downloads/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-23 18:09:47.348 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IUjZMZXtb89p",
        "89m2jMJMcJky",
        "EGRf5xGxcaHF",
        "YGxpVfS4ccbj",
        "Br3NWTQEcnXL",
        "bb-UPIUPczCA",
        "hcMJpEuZLzPh",
        "Qa0czko8L2Iv",
        "rrMdYc1dG7eO",
        "msN4Q1KmT5Za",
        "DavKW8gHT1vg",
        "jjcuynC5jpoi",
        "bhyIG3at48Wp",
        "4KxssxMlvDsV"
      ],
      "name": "GramEval_poetry_main_2305.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}